{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1431a090-6740-49aa-b483-1a783990a521"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 10 06:33:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\" -O CUB_200_2011.tgz && rm -rf /tmp/cookies.txt\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_60.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e8f3820-d1c4-44bd-9221-6d416562cfbd"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:02<00:00, 39.4MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  122.9880781173706\n",
            "step :  200 iters_100_time :  123.50126004219055\n",
            "step :  300 iters_100_time :  123.51646280288696\n",
            "step :  400 iters_100_time :  123.55942392349243\n",
            "[0/600][442]\n",
            "                    Loss_D: 1.99 Loss_G: 69.76 Time: 545.59s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  72.19356393814087\n",
            "step :  600 iters_100_time :  123.51399803161621\n",
            "step :  700 iters_100_time :  123.73732995986938\n",
            "step :  800 iters_100_time :  123.55360317230225\n",
            "[1/600][442]\n",
            "                    Loss_D: 1.56 Loss_G: 61.20 Time: 547.09s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  20.070841550827026\n",
            "step :  1000 iters_100_time :  123.5865957736969\n",
            "step :  1100 iters_100_time :  123.61403298377991\n",
            "step :  1200 iters_100_time :  123.68813228607178\n",
            "step :  1300 iters_100_time :  123.66901516914368\n",
            "[2/600][442]\n",
            "                    Loss_D: 1.28 Loss_G: 59.59 Time: 547.16s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  91.85583424568176\n",
            "step :  1500 iters_100_time :  123.85957336425781\n",
            "step :  1600 iters_100_time :  123.64059376716614\n",
            "step :  1700 iters_100_time :  123.66083765029907\n",
            "[3/600][442]\n",
            "                    Loss_D: 1.63 Loss_G: 50.83 Time: 547.46s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  40.092554330825806\n",
            "step :  1900 iters_100_time :  123.69668507575989\n",
            "step :  2000 iters_100_time :  123.56824612617493\n",
            "step :  2100 iters_100_time :  123.60023546218872\n",
            "step :  2200 iters_100_time :  123.63229584693909\n",
            "[4/600][442]\n",
            "                    Loss_D: 0.69 Loss_G: 64.95 Time: 547.42s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  111.61233854293823\n",
            "step :  2400 iters_100_time :  123.69164967536926\n",
            "step :  2500 iters_100_time :  123.6533317565918\n",
            "step :  2600 iters_100_time :  123.58288598060608\n",
            "[5/600][442]\n",
            "                    Loss_D: 0.94 Loss_G: 45.81 Time: 547.29s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  59.668872594833374\n",
            "step :  2800 iters_100_time :  123.84415030479431\n",
            "step :  2900 iters_100_time :  123.88059091567993\n",
            "step :  3000 iters_100_time :  123.80326509475708\n",
            "[6/600][442]\n",
            "                    Loss_D: 0.64 Loss_G: 57.40 Time: 548.07s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  7.737273454666138\n",
            "step :  3200 iters_100_time :  123.74568295478821\n",
            "step :  3300 iters_100_time :  123.56778573989868\n",
            "step :  3400 iters_100_time :  123.69432091712952\n",
            "step :  3500 iters_100_time :  123.63439774513245\n",
            "[7/600][442]\n",
            "                    Loss_D: 2.67 Loss_G: 45.21 Time: 547.33s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  79.25946021080017\n",
            "step :  3700 iters_100_time :  123.58021473884583\n",
            "step :  3800 iters_100_time :  123.50408673286438\n",
            "step :  3900 iters_100_time :  123.4052996635437\n",
            "[8/600][442]\n",
            "                    Loss_D: 3.42 Loss_G: 54.95 Time: 546.32s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  27.519819974899292\n",
            "step :  4100 iters_100_time :  123.53527426719666\n",
            "step :  4200 iters_100_time :  123.5714180469513\n",
            "step :  4300 iters_100_time :  123.617835521698\n",
            "step :  4400 iters_100_time :  123.655996799469\n",
            "[9/600][442]\n",
            "                    Loss_D: 0.39 Loss_G: 44.38 Time: 547.02s\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  99.29239678382874\n",
            "step :  4600 iters_100_time :  123.64500570297241\n",
            "step :  4700 iters_100_time :  123.47027158737183\n",
            "step :  4800 iters_100_time :  123.5645821094513\n",
            "[10/600][442]\n",
            "                    Loss_D: 0.67 Loss_G: 54.69 Time: 546.87s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  47.535077810287476\n",
            "step :  5000 iters_100_time :  123.29853749275208\n",
            "step :  5100 iters_100_time :  123.40876936912537\n",
            "step :  5200 iters_100_time :  123.54359245300293\n",
            "step :  5300 iters_100_time :  123.55610132217407\n",
            "[11/600][442]\n",
            "                    Loss_D: 0.74 Loss_G: 51.20 Time: 546.81s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  119.11042881011963\n",
            "step :  5500 iters_100_time :  123.68761873245239\n",
            "step :  5600 iters_100_time :  123.63612198829651\n",
            "step :  5700 iters_100_time :  123.69056558609009\n",
            "[12/600][442]\n",
            "                    Loss_D: 0.77 Loss_G: 47.10 Time: 547.26s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  67.20041298866272\n",
            "step :  5900 iters_100_time :  123.8007025718689\n",
            "step :  6000 iters_100_time :  123.89265084266663\n",
            "step :  6100 iters_100_time :  123.89777851104736\n",
            "[13/600][442]\n",
            "                    Loss_D: 0.78 Loss_G: 52.13 Time: 548.05s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  15.246437311172485\n",
            "step :  6300 iters_100_time :  123.76054644584656\n",
            "step :  6400 iters_100_time :  123.80123996734619\n",
            "step :  6500 iters_100_time :  123.70246028900146\n",
            "step :  6600 iters_100_time :  123.78491139411926\n",
            "[14/600][442]\n",
            "                    Loss_D: 0.26 Loss_G: 50.92 Time: 547.72s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  87.09103584289551\n",
            "step :  6800 iters_100_time :  123.75745439529419\n",
            "step :  6900 iters_100_time :  123.75062704086304\n",
            "step :  7000 iters_100_time :  123.82808423042297\n",
            "[15/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 44.10 Time: 548.07s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  35.120970726013184\n",
            "step :  7200 iters_100_time :  123.6310625076294\n",
            "step :  7300 iters_100_time :  123.88628315925598\n",
            "step :  7400 iters_100_time :  123.83494997024536\n",
            "step :  7500 iters_100_time :  123.6972074508667\n",
            "[16/600][442]\n",
            "                    Loss_D: 0.45 Loss_G: 56.81 Time: 547.89s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  106.87338256835938\n",
            "step :  7700 iters_100_time :  123.93097996711731\n",
            "step :  7800 iters_100_time :  123.76343035697937\n",
            "step :  7900 iters_100_time :  123.67478394508362\n",
            "[17/600][442]\n",
            "                    Loss_D: 0.73 Loss_G: 46.99 Time: 547.90s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  54.8638219833374\n",
            "step :  8100 iters_100_time :  123.89330816268921\n",
            "step :  8200 iters_100_time :  123.72893047332764\n",
            "step :  8300 iters_100_time :  123.7499930858612\n",
            "[18/600][442]\n",
            "                    Loss_D: 0.37 Loss_G: 62.43 Time: 547.88s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.854503631591797\n",
            "step :  8500 iters_100_time :  123.76416635513306\n",
            "step :  8600 iters_100_time :  123.70497536659241\n",
            "step :  8700 iters_100_time :  123.72325348854065\n",
            "step :  8800 iters_100_time :  123.82225179672241\n",
            "[19/600][442]\n",
            "                    Loss_D: 0.53 Loss_G: 44.91 Time: 547.76s\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  74.7621967792511\n",
            "step :  9000 iters_100_time :  123.86289238929749\n",
            "step :  9100 iters_100_time :  123.76603245735168\n",
            "step :  9200 iters_100_time :  123.74978566169739\n",
            "[20/600][442]\n",
            "                    Loss_D: 0.58 Loss_G: 41.66 Time: 548.02s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  22.73348641395569\n",
            "step :  9400 iters_100_time :  123.99854326248169\n",
            "step :  9500 iters_100_time :  123.99318504333496\n",
            "step :  9600 iters_100_time :  124.00963473320007\n",
            "step :  9700 iters_100_time :  123.72055077552795\n",
            "[21/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 45.96 Time: 548.61s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  94.55225038528442\n",
            "step :  9900 iters_100_time :  123.83139896392822\n",
            "step :  10000 iters_100_time :  123.89936065673828\n",
            "step :  10100 iters_100_time :  124.01160049438477\n",
            "[22/600][442]\n",
            "                    Loss_D: 0.55 Loss_G: 56.82 Time: 548.19s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  42.450417041778564\n",
            "step :  10300 iters_100_time :  123.76482820510864\n",
            "step :  10400 iters_100_time :  123.81388306617737\n",
            "step :  10500 iters_100_time :  123.83723521232605\n",
            "step :  10600 iters_100_time :  123.76226949691772\n",
            "[23/600][442]\n",
            "                    Loss_D: 1.29 Loss_G: 40.89 Time: 547.90s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  114.1032886505127\n",
            "step :  10800 iters_100_time :  123.73767423629761\n",
            "step :  10900 iters_100_time :  123.71982169151306\n",
            "step :  11000 iters_100_time :  123.63668489456177\n",
            "[24/600][442]\n",
            "                    Loss_D: 0.61 Loss_G: 34.58 Time: 547.34s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  62.299970626831055\n",
            "step :  11200 iters_100_time :  123.75061321258545\n",
            "step :  11300 iters_100_time :  123.69957137107849\n",
            "step :  11400 iters_100_time :  123.62600636482239\n",
            "[25/600][442]\n",
            "                    Loss_D: 0.59 Loss_G: 36.47 Time: 547.54s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  10.335124969482422\n",
            "step :  11600 iters_100_time :  123.86770582199097\n",
            "step :  11700 iters_100_time :  123.63665580749512\n",
            "step :  11800 iters_100_time :  123.65229558944702\n",
            "step :  11900 iters_100_time :  123.7839150428772\n",
            "[26/600][442]\n",
            "                    Loss_D: 0.59 Loss_G: 40.29 Time: 547.72s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  81.92981839179993\n",
            "step :  12100 iters_100_time :  123.7500913143158\n",
            "step :  12200 iters_100_time :  123.67487239837646\n",
            "step :  12300 iters_100_time :  123.98998355865479\n",
            "[27/600][442]\n",
            "                    Loss_D: 0.49 Loss_G: 46.76 Time: 547.79s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  30.099345445632935\n",
            "step :  12500 iters_100_time :  123.73151206970215\n",
            "step :  12600 iters_100_time :  123.7555582523346\n",
            "step :  12700 iters_100_time :  123.89774656295776\n",
            "step :  12800 iters_100_time :  123.70620489120483\n",
            "[28/600][442]\n",
            "                    Loss_D: 0.55 Loss_G: 41.04 Time: 547.98s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  102.04705834388733\n",
            "step :  13000 iters_100_time :  124.0101249217987\n",
            "step :  13100 iters_100_time :  123.70482444763184\n",
            "step :  13200 iters_100_time :  123.71807956695557\n",
            "[29/600][442]\n",
            "                    Loss_D: 1.84 Loss_G: 65.09 Time: 548.07s\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  49.941537380218506\n",
            "step :  13400 iters_100_time :  123.91385865211487\n",
            "step :  13500 iters_100_time :  123.81414246559143\n",
            "step :  13600 iters_100_time :  123.69103503227234\n",
            "step :  13700 iters_100_time :  123.82711863517761\n",
            "[30/600][442]\n",
            "                    Loss_D: 0.47 Loss_G: 44.85 Time: 548.13s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  121.85420441627502\n",
            "step :  13900 iters_100_time :  123.5934362411499\n",
            "step :  14000 iters_100_time :  123.75664448738098\n",
            "step :  14100 iters_100_time :  123.65758728981018\n",
            "[31/600][442]\n",
            "                    Loss_D: 1.34 Loss_G: 62.34 Time: 547.76s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  69.79214119911194\n",
            "step :  14300 iters_100_time :  123.88921809196472\n",
            "step :  14400 iters_100_time :  123.67030429840088\n",
            "step :  14500 iters_100_time :  123.80263471603394\n",
            "[32/600][442]\n",
            "                    Loss_D: 0.58 Loss_G: 55.63 Time: 547.92s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  17.800009727478027\n",
            "step :  14700 iters_100_time :  123.73806548118591\n",
            "step :  14800 iters_100_time :  123.70879125595093\n",
            "step :  14900 iters_100_time :  123.74097871780396\n",
            "step :  15000 iters_100_time :  123.7526466846466\n",
            "[33/600][442]\n",
            "                    Loss_D: 1.27 Loss_G: 64.66 Time: 547.80s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  89.33240103721619\n",
            "step :  15200 iters_100_time :  123.9001259803772\n",
            "step :  15300 iters_100_time :  123.81867623329163\n",
            "step :  15400 iters_100_time :  123.90309619903564\n",
            "[34/600][442]\n",
            "                    Loss_D: 2.07 Loss_G: 70.66 Time: 548.07s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  37.75133657455444\n",
            "step :  15600 iters_100_time :  123.68201446533203\n",
            "step :  15700 iters_100_time :  123.80445861816406\n",
            "step :  15800 iters_100_time :  123.78062081336975\n",
            "step :  15900 iters_100_time :  123.69931674003601\n",
            "[35/600][442]\n",
            "                    Loss_D: 1.17 Loss_G: 31.02 Time: 548.03s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  109.16648697853088\n",
            "step :  16100 iters_100_time :  123.67453217506409\n",
            "step :  16200 iters_100_time :  123.72068786621094\n",
            "step :  16300 iters_100_time :  123.7230613231659\n",
            "[36/600][442]\n",
            "                    Loss_D: 1.25 Loss_G: 56.61 Time: 547.49s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  57.366466760635376\n",
            "step :  16500 iters_100_time :  123.74731731414795\n",
            "step :  16600 iters_100_time :  123.80198240280151\n",
            "step :  16700 iters_100_time :  123.7850193977356\n",
            "[37/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 48.18 Time: 547.82s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  5.19826078414917\n",
            "step :  16900 iters_100_time :  123.81522226333618\n",
            "step :  17000 iters_100_time :  123.57294821739197\n",
            "step :  17100 iters_100_time :  123.59058380126953\n",
            "step :  17200 iters_100_time :  123.57048797607422\n",
            "[38/600][442]\n",
            "                    Loss_D: 0.44 Loss_G: 35.30 Time: 547.26s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  77.08054518699646\n",
            "step :  17400 iters_100_time :  123.86662697792053\n",
            "step :  17500 iters_100_time :  123.90571188926697\n",
            "step :  17600 iters_100_time :  123.78978681564331\n",
            "[39/600][442]\n",
            "                    Loss_D: 1.39 Loss_G: 36.30 Time: 547.93s\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  25.192363023757935\n",
            "step :  17800 iters_100_time :  123.68232250213623\n",
            "step :  17900 iters_100_time :  123.62863874435425\n",
            "step :  18000 iters_100_time :  123.65119552612305\n",
            "step :  18100 iters_100_time :  123.6037175655365\n",
            "[40/600][442]\n",
            "                    Loss_D: 0.77 Loss_G: 50.82 Time: 547.32s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  97.30206656455994\n",
            "step :  18300 iters_100_time :  123.67224311828613\n",
            "step :  18400 iters_100_time :  123.82879185676575\n",
            "step :  18500 iters_100_time :  123.98200535774231\n",
            "[41/600][442]\n",
            "                    Loss_D: 0.36 Loss_G: 47.98 Time: 548.40s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  45.07296109199524\n",
            "step :  18700 iters_100_time :  123.83934235572815\n",
            "step :  18800 iters_100_time :  123.8340790271759\n",
            "step :  18900 iters_100_time :  123.92910981178284\n",
            "step :  19000 iters_100_time :  123.66853618621826\n",
            "[42/600][442]\n",
            "                    Loss_D: 0.29 Loss_G: 47.55 Time: 548.13s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  117.05165791511536\n",
            "step :  19200 iters_100_time :  123.83673286437988\n",
            "step :  19300 iters_100_time :  123.76670789718628\n",
            "step :  19400 iters_100_time :  123.76404666900635\n",
            "[43/600][442]\n",
            "                    Loss_D: 0.45 Loss_G: 43.13 Time: 548.25s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  64.60049319267273\n",
            "step :  19600 iters_100_time :  123.83237886428833\n",
            "step :  19700 iters_100_time :  123.79360675811768\n",
            "step :  19800 iters_100_time :  123.80140399932861\n",
            "[44/600][442]\n",
            "                    Loss_D: 1.55 Loss_G: 49.83 Time: 547.93s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  12.713602304458618\n",
            "step :  20000 iters_100_time :  123.81434297561646\n",
            "step :  20100 iters_100_time :  123.73663759231567\n",
            "step :  20200 iters_100_time :  123.83233451843262\n",
            "step :  20300 iters_100_time :  123.9530577659607\n",
            "[45/600][442]\n",
            "                    Loss_D: 1.12 Loss_G: 52.16 Time: 548.03s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  84.60549426078796\n",
            "step :  20500 iters_100_time :  123.76305818557739\n",
            "step :  20600 iters_100_time :  123.89244198799133\n",
            "step :  20700 iters_100_time :  123.93174624443054\n",
            "[46/600][442]\n",
            "                    Loss_D: 2.06 Loss_G: 69.66 Time: 548.22s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  32.66576313972473\n",
            "step :  20900 iters_100_time :  123.92678236961365\n",
            "step :  21000 iters_100_time :  123.77531695365906\n",
            "step :  21100 iters_100_time :  124.14322257041931\n",
            "step :  21200 iters_100_time :  123.8270013332367\n",
            "[47/600][442]\n",
            "                    Loss_D: 1.78 Loss_G: 60.04 Time: 548.55s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  104.24895143508911\n",
            "step :  21400 iters_100_time :  123.94970440864563\n",
            "step :  21500 iters_100_time :  123.87637376785278\n",
            "step :  21600 iters_100_time :  123.76838755607605\n",
            "[48/600][442]\n",
            "                    Loss_D: 0.83 Loss_G: 62.07 Time: 547.98s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  52.34942269325256\n",
            "step :  21800 iters_100_time :  123.7589967250824\n",
            "step :  21900 iters_100_time :  123.78571200370789\n",
            "step :  22000 iters_100_time :  123.73588037490845\n",
            "step :  22100 iters_100_time :  123.8383858203888\n",
            "[49/600][442]\n",
            "                    Loss_D: 2.21 Loss_G: 54.51 Time: 547.83s\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  124.23506498336792\n",
            "step :  22300 iters_100_time :  123.80292510986328\n",
            "step :  22400 iters_100_time :  123.79849481582642\n",
            "step :  22500 iters_100_time :  123.67508125305176\n",
            "[50/600][442]\n",
            "                    Loss_D: 0.22 Loss_G: 46.45 Time: 548.03s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  72.45616054534912\n",
            "step :  22700 iters_100_time :  123.62035465240479\n",
            "step :  22800 iters_100_time :  123.69283676147461\n",
            "step :  22900 iters_100_time :  123.86797165870667\n",
            "[51/600][442]\n",
            "                    Loss_D: 2.75 Loss_G: 30.27 Time: 547.93s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  20.18803381919861\n",
            "step :  23100 iters_100_time :  123.99880361557007\n",
            "step :  23200 iters_100_time :  123.72019720077515\n",
            "step :  23300 iters_100_time :  123.77007150650024\n",
            "step :  23400 iters_100_time :  123.82271194458008\n",
            "[52/600][442]\n",
            "                    Loss_D: 0.51 Loss_G: 40.76 Time: 548.14s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  91.8869857788086\n",
            "step :  23600 iters_100_time :  123.8256504535675\n",
            "step :  23700 iters_100_time :  123.54111576080322\n",
            "step :  23800 iters_100_time :  123.74998307228088\n",
            "[53/600][442]\n",
            "                    Loss_D: 0.22 Loss_G: 43.60 Time: 547.50s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  39.88799238204956\n",
            "step :  24000 iters_100_time :  123.7406976222992\n",
            "step :  24100 iters_100_time :  123.77936744689941\n",
            "step :  24200 iters_100_time :  123.58386015892029\n",
            "step :  24300 iters_100_time :  123.70425772666931\n",
            "[54/600][442]\n",
            "                    Loss_D: 0.99 Loss_G: 39.21 Time: 547.47s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  111.75804829597473\n",
            "step :  24500 iters_100_time :  123.78519296646118\n",
            "step :  24600 iters_100_time :  123.77042531967163\n",
            "step :  24700 iters_100_time :  123.75128293037415\n",
            "[55/600][442]\n",
            "                    Loss_D: 0.46 Loss_G: 39.09 Time: 547.89s\n",
            "num_batches :  442\n",
            "step :  24800 iters_100_time :  59.852256774902344\n",
            "step :  24900 iters_100_time :  123.55883622169495\n",
            "step :  25000 iters_100_time :  123.85907244682312\n",
            "step :  25000 iters_5000_time :  307.27029609680176\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.2922070026397705\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  27.403764009475708\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  219.91506028175354\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  25100 iters_100_time :  400.8911225795746\n",
            "[56/600][442]\n",
            "                    Loss_D: 2.75 Loss_G: 56.26 Time: 824.90s\n",
            "num_batches :  442\n",
            "step :  25200 iters_100_time :  7.830754280090332\n",
            "step :  25300 iters_100_time :  123.82517528533936\n",
            "step :  25400 iters_100_time :  123.88160371780396\n",
            "step :  25500 iters_100_time :  123.68026041984558\n",
            "step :  25600 iters_100_time :  124.02010464668274\n",
            "[57/600][442]\n",
            "                    Loss_D: 0.59 Loss_G: 52.43 Time: 548.19s\n",
            "num_batches :  442\n",
            "step :  25700 iters_100_time :  79.58922410011292\n",
            "step :  25800 iters_100_time :  123.85881018638611\n",
            "step :  25900 iters_100_time :  123.78969144821167\n",
            "step :  26000 iters_100_time :  123.48158073425293\n",
            "[58/600][442]\n",
            "                    Loss_D: 1.37 Loss_G: 50.99 Time: 547.80s\n",
            "num_batches :  442\n",
            "step :  26100 iters_100_time :  27.46787428855896\n",
            "step :  26200 iters_100_time :  123.5920147895813\n",
            "step :  26300 iters_100_time :  123.71380639076233\n",
            "step :  26400 iters_100_time :  123.82624292373657\n",
            "step :  26500 iters_100_time :  123.5444085597992\n",
            "[59/600][442]\n",
            "                    Loss_D: 0.26 Loss_G: 54.54 Time: 547.29s\n",
            "num_batches :  442\n",
            "step :  26600 iters_100_time :  99.51137208938599\n",
            "step :  26700 iters_100_time :  123.63608598709106\n",
            "step :  26800 iters_100_time :  123.76028656959534\n",
            "step :  26900 iters_100_time :  123.9677369594574\n",
            "[60/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 41.54 Time: 548.14s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  27000 iters_100_time :  47.764286518096924\n",
            "step :  27100 iters_100_time :  123.85879445075989\n",
            "step :  27200 iters_100_time :  123.81571817398071\n",
            "step :  27300 iters_100_time :  123.79193353652954\n",
            "step :  27400 iters_100_time :  124.10964179039001\n",
            "[61/600][442]\n",
            "                    Loss_D: 2.06 Loss_G: 58.37 Time: 548.73s\n",
            "num_batches :  442\n",
            "step :  27500 iters_100_time :  119.43278670310974\n",
            "step :  27600 iters_100_time :  123.91495084762573\n",
            "step :  27700 iters_100_time :  123.79970335960388\n",
            "step :  27800 iters_100_time :  123.5092887878418\n",
            "[62/600][442]\n",
            "                    Loss_D: 0.52 Loss_G: 39.06 Time: 548.13s\n",
            "num_batches :  442\n",
            "step :  27900 iters_100_time :  67.12823271751404\n",
            "step :  28000 iters_100_time :  123.61254549026489\n",
            "step :  28100 iters_100_time :  123.65375065803528\n",
            "step :  28200 iters_100_time :  123.6671667098999\n",
            "[63/600][442]\n",
            "                    Loss_D: 1.33 Loss_G: 62.55 Time: 547.33s\n",
            "num_batches :  442\n",
            "step :  28300 iters_100_time :  15.106519222259521\n",
            "step :  28400 iters_100_time :  123.74709153175354\n",
            "step :  28500 iters_100_time :  123.62831568717957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU78aNl1gQdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fbFB2yZuaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}