{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_520.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6fa3363e-9bd1-4438-c045-86461f786471"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_460.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:02<00:00, 45.6MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_460.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS =========  461\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  82.59548783302307\n",
            "step :  200 iters_100_time :  77.48345422744751\n",
            "step :  300 iters_100_time :  77.7765064239502\n",
            "step :  400 iters_100_time :  78.00230741500854\n",
            "[461/600][442]\n",
            "                    Loss_D: 0.68 Loss_G: 56.20 Time: 348.46s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  45.23745536804199\n",
            "step :  600 iters_100_time :  78.0136330127716\n",
            "step :  700 iters_100_time :  77.9898271560669\n",
            "step :  800 iters_100_time :  76.9259569644928\n",
            "[462/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 69.57 Time: 343.71s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  13.071507930755615\n",
            "step :  1000 iters_100_time :  78.06661534309387\n",
            "step :  1100 iters_100_time :  77.77329993247986\n",
            "step :  1200 iters_100_time :  78.03892135620117\n",
            "step :  1300 iters_100_time :  78.06173968315125\n",
            "[463/600][442]\n",
            "                    Loss_D: 0.65 Loss_G: 51.29 Time: 345.36s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  58.372347831726074\n",
            "step :  1500 iters_100_time :  78.04386758804321\n",
            "step :  1600 iters_100_time :  77.58826661109924\n",
            "step :  1700 iters_100_time :  78.47442364692688\n",
            "[464/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 69.27 Time: 345.93s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  26.100613832473755\n",
            "step :  1900 iters_100_time :  77.3298192024231\n",
            "step :  2000 iters_100_time :  77.91790533065796\n",
            "step :  2100 iters_100_time :  78.4240128993988\n",
            "step :  2200 iters_100_time :  78.41440677642822\n",
            "[465/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 64.96 Time: 346.43s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  71.30138063430786\n",
            "step :  2400 iters_100_time :  78.51161026954651\n",
            "step :  2500 iters_100_time :  77.89239263534546\n",
            "step :  2600 iters_100_time :  78.45945763587952\n",
            "[466/600][442]\n",
            "                    Loss_D: 0.22 Loss_G: 78.36 Time: 347.12s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  38.40752148628235\n",
            "step :  2800 iters_100_time :  78.46868944168091\n",
            "step :  2900 iters_100_time :  78.33215689659119\n",
            "step :  3000 iters_100_time :  78.4060959815979\n",
            "[467/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 74.21 Time: 347.50s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  5.26940131187439\n",
            "step :  3200 iters_100_time :  78.2351348400116\n",
            "step :  3300 iters_100_time :  78.25214791297913\n",
            "step :  3400 iters_100_time :  78.76924347877502\n",
            "step :  3500 iters_100_time :  78.49075675010681\n",
            "[468/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 58.08 Time: 346.90s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  51.047842025756836\n",
            "step :  3700 iters_100_time :  78.52435731887817\n",
            "step :  3800 iters_100_time :  78.61190676689148\n",
            "step :  3900 iters_100_time :  78.52945899963379\n",
            "[469/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 53.49 Time: 348.80s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  17.887337684631348\n",
            "step :  4100 iters_100_time :  78.76583743095398\n",
            "step :  4200 iters_100_time :  77.97225022315979\n",
            "step :  4300 iters_100_time :  79.05171489715576\n",
            "step :  4400 iters_100_time :  78.65228915214539\n",
            "[470/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 68.15 Time: 348.13s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  63.95742583274841\n",
            "step :  4600 iters_100_time :  78.44750595092773\n",
            "step :  4700 iters_100_time :  78.18217587471008\n",
            "step :  4800 iters_100_time :  77.72930359840393\n",
            "[471/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 69.51 Time: 347.91s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  30.06833815574646\n",
            "step :  5000 iters_100_time :  79.27783274650574\n",
            "step :  5100 iters_100_time :  78.34388947486877\n",
            "step :  5200 iters_100_time :  78.5007631778717\n",
            "step :  5300 iters_100_time :  78.39377760887146\n",
            "[472/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 69.55 Time: 347.90s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  76.12820935249329\n",
            "step :  5500 iters_100_time :  78.79991626739502\n",
            "step :  5600 iters_100_time :  78.75455951690674\n",
            "step :  5700 iters_100_time :  77.8164553642273\n",
            "[473/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 59.03 Time: 348.43s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  42.94962739944458\n",
            "step :  5900 iters_100_time :  79.13525891304016\n",
            "step :  6000 iters_100_time :  78.42646193504333\n",
            "step :  6100 iters_100_time :  78.96879267692566\n",
            "[474/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 60.99 Time: 348.53s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  10.002916097640991\n",
            "step :  6300 iters_100_time :  78.34049534797668\n",
            "step :  6400 iters_100_time :  79.03284549713135\n",
            "step :  6500 iters_100_time :  78.64503359794617\n",
            "step :  6600 iters_100_time :  79.24727606773376\n",
            "[475/600][442]\n",
            "                    Loss_D: 0.13 Loss_G: 90.09 Time: 348.76s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  56.23444175720215\n",
            "step :  6800 iters_100_time :  78.38752675056458\n",
            "step :  6900 iters_100_time :  79.02076363563538\n",
            "step :  7000 iters_100_time :  79.30767965316772\n",
            "[476/600][442]\n",
            "                    Loss_D: 0.33 Loss_G: 42.37 Time: 349.77s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  22.961079120635986\n",
            "step :  7200 iters_100_time :  78.77765154838562\n",
            "step :  7300 iters_100_time :  78.7045521736145\n",
            "step :  7400 iters_100_time :  78.92537999153137\n",
            "step :  7500 iters_100_time :  79.06389689445496\n",
            "[477/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 72.39 Time: 349.63s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  68.78629899024963\n",
            "step :  7700 iters_100_time :  77.88729166984558\n",
            "step :  7800 iters_100_time :  78.59391164779663\n",
            "step :  7900 iters_100_time :  79.4294970035553\n",
            "[478/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 77.70 Time: 349.22s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  35.34042501449585\n",
            "step :  8100 iters_100_time :  78.81007885932922\n",
            "step :  8200 iters_100_time :  78.74982810020447\n",
            "step :  8300 iters_100_time :  78.5654571056366\n",
            "[479/600][442]\n",
            "                    Loss_D: 0.65 Loss_G: 67.70 Time: 348.78s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.332683801651001\n",
            "step :  8500 iters_100_time :  79.07980942726135\n",
            "step :  8600 iters_100_time :  79.18805742263794\n",
            "step :  8700 iters_100_time :  78.67796206474304\n",
            "step :  8800 iters_100_time :  78.25234293937683\n",
            "[480/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 50.15 Time: 349.53s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  47.494794607162476\n",
            "step :  9000 iters_100_time :  79.71396040916443\n",
            "step :  9100 iters_100_time :  78.8800139427185\n",
            "step :  9200 iters_100_time :  78.39216494560242\n",
            "[481/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 87.76 Time: 349.57s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  14.813787698745728\n",
            "step :  9400 iters_100_time :  78.8770842552185\n",
            "step :  9500 iters_100_time :  78.61669254302979\n",
            "step :  9600 iters_100_time :  79.49245071411133\n",
            "step :  9700 iters_100_time :  78.90347146987915\n",
            "[482/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 57.60 Time: 349.70s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  60.766560077667236\n",
            "step :  9900 iters_100_time :  78.88494920730591\n",
            "step :  10000 iters_100_time :  78.49749827384949\n",
            "step :  10100 iters_100_time :  78.72785973548889\n",
            "[483/600][442]\n",
            "                    Loss_D: 0.29 Loss_G: 48.72 Time: 349.36s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  27.90921902656555\n",
            "step :  10300 iters_100_time :  78.80784225463867\n",
            "step :  10400 iters_100_time :  78.75481629371643\n",
            "step :  10500 iters_100_time :  78.84632992744446\n",
            "step :  10600 iters_100_time :  78.3851523399353\n",
            "[484/600][442]\n",
            "                    Loss_D: 0.95 Loss_G: 66.16 Time: 349.54s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  73.66892266273499\n",
            "step :  10800 iters_100_time :  78.99199390411377\n",
            "step :  10900 iters_100_time :  78.3937017917633\n",
            "step :  11000 iters_100_time :  78.97027373313904\n",
            "[485/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 60.99 Time: 350.06s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  39.25272512435913\n",
            "step :  11200 iters_100_time :  79.03471565246582\n",
            "step :  11300 iters_100_time :  79.23671698570251\n",
            "step :  11400 iters_100_time :  79.02281379699707\n",
            "[486/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 47.31 Time: 349.24s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  6.872532606124878\n",
            "step :  11600 iters_100_time :  79.06548428535461\n",
            "step :  11700 iters_100_time :  78.71369099617004\n",
            "step :  11800 iters_100_time :  78.85792183876038\n",
            "step :  11900 iters_100_time :  79.20319867134094\n",
            "[487/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 63.72 Time: 349.35s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  52.85912728309631\n",
            "step :  12100 iters_100_time :  78.79123425483704\n",
            "step :  12200 iters_100_time :  79.00189137458801\n",
            "step :  12300 iters_100_time :  79.34583783149719\n",
            "[488/600][442]\n",
            "                    Loss_D: 0.51 Loss_G: 68.20 Time: 350.44s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  19.587576627731323\n",
            "step :  12500 iters_100_time :  78.99139380455017\n",
            "step :  12600 iters_100_time :  78.63491106033325\n",
            "step :  12700 iters_100_time :  78.88854193687439\n",
            "step :  12800 iters_100_time :  79.72797894477844\n",
            "[489/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 53.67 Time: 349.97s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  65.49617719650269\n",
            "step :  13000 iters_100_time :  79.53683114051819\n",
            "step :  13100 iters_100_time :  78.78636169433594\n",
            "step :  13200 iters_100_time :  78.44573307037354\n",
            "[490/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 67.81 Time: 350.23s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  32.40363264083862\n",
            "step :  13400 iters_100_time :  79.65680861473083\n",
            "step :  13500 iters_100_time :  78.7601728439331\n",
            "step :  13600 iters_100_time :  79.26731324195862\n",
            "step :  13700 iters_100_time :  79.06926012039185\n",
            "[491/600][442]\n",
            "                    Loss_D: 0.26 Loss_G: 79.46 Time: 350.95s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  77.99926471710205\n",
            "step :  13900 iters_100_time :  79.28959488868713\n",
            "step :  14000 iters_100_time :  79.61505174636841\n",
            "step :  14100 iters_100_time :  79.23536896705627\n",
            "[492/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 47.65 Time: 351.27s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  45.13615894317627\n",
            "step :  14300 iters_100_time :  79.0197012424469\n",
            "step :  14400 iters_100_time :  79.2406792640686\n",
            "step :  14500 iters_100_time :  79.48668193817139\n",
            "[493/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 61.96 Time: 351.44s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  11.747608423233032\n",
            "step :  14700 iters_100_time :  79.26664876937866\n",
            "step :  14800 iters_100_time :  78.93477058410645\n",
            "step :  14900 iters_100_time :  78.69263577461243\n",
            "step :  15000 iters_100_time :  79.4700095653534\n",
            "[494/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 82.62 Time: 351.14s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  58.0641930103302\n",
            "step :  15200 iters_100_time :  78.84975600242615\n",
            "step :  15300 iters_100_time :  79.13227105140686\n",
            "step :  15400 iters_100_time :  78.94307088851929\n",
            "[495/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 70.03 Time: 350.97s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  24.37412691116333\n",
            "step :  15600 iters_100_time :  79.39171624183655\n",
            "step :  15700 iters_100_time :  79.3330762386322\n",
            "step :  15800 iters_100_time :  79.00961709022522\n",
            "step :  15900 iters_100_time :  79.30144429206848\n",
            "[496/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 72.95 Time: 351.01s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  71.18157768249512\n",
            "step :  16100 iters_100_time :  79.19944024085999\n",
            "step :  16200 iters_100_time :  79.4746356010437\n",
            "step :  16300 iters_100_time :  79.14140963554382\n",
            "[497/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 77.17 Time: 352.18s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  37.09108114242554\n",
            "step :  16500 iters_100_time :  79.08613061904907\n",
            "step :  16600 iters_100_time :  79.45141363143921\n",
            "step :  16700 iters_100_time :  79.30151414871216\n",
            "[498/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 60.91 Time: 351.38s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  4.162564754486084\n",
            "step :  16900 iters_100_time :  78.71239447593689\n",
            "step :  17000 iters_100_time :  78.93867707252502\n",
            "step :  17100 iters_100_time :  79.47272825241089\n",
            "step :  17200 iters_100_time :  79.119220495224\n",
            "[499/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 69.26 Time: 351.24s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  49.95731830596924\n",
            "step :  17400 iters_100_time :  79.1521828174591\n",
            "step :  17500 iters_100_time :  78.89820098876953\n",
            "step :  17600 iters_100_time :  79.39349603652954\n",
            "[500/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 72.58 Time: 351.41s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  17.04668927192688\n",
            "step :  17800 iters_100_time :  79.36893701553345\n",
            "step :  17900 iters_100_time :  79.5240707397461\n",
            "step :  18000 iters_100_time :  79.35474467277527\n",
            "step :  18100 iters_100_time :  78.64691472053528\n",
            "[501/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 68.74 Time: 352.57s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  62.54995679855347\n",
            "step :  18300 iters_100_time :  79.57090902328491\n",
            "step :  18400 iters_100_time :  79.00294709205627\n",
            "step :  18500 iters_100_time :  79.37076950073242\n",
            "[502/600][442]\n",
            "                    Loss_D: 1.98 Loss_G: 62.74 Time: 351.64s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  29.16803503036499\n",
            "step :  18700 iters_100_time :  79.4800636768341\n",
            "step :  18800 iters_100_time :  79.46600890159607\n",
            "step :  18900 iters_100_time :  79.22038555145264\n",
            "step :  19000 iters_100_time :  79.04913187026978\n",
            "[503/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 54.66 Time: 351.41s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  75.22244310379028\n",
            "step :  19200 iters_100_time :  80.02197217941284\n",
            "step :  19300 iters_100_time :  79.28269720077515\n",
            "step :  19400 iters_100_time :  79.6823377609253\n",
            "[504/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 62.82 Time: 352.88s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  41.638285875320435\n",
            "step :  19600 iters_100_time :  79.40169906616211\n",
            "step :  19700 iters_100_time :  80.11105513572693\n",
            "step :  19800 iters_100_time :  79.44351148605347\n",
            "[505/600][442]\n",
            "                    Loss_D: 0.16 Loss_G: 54.82 Time: 352.59s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  8.907295942306519\n",
            "step :  20000 iters_100_time :  79.41199469566345\n",
            "step :  20100 iters_100_time :  79.04133462905884\n",
            "step :  20200 iters_100_time :  80.45641541481018\n",
            "step :  20300 iters_100_time :  79.7299211025238\n",
            "[506/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 64.18 Time: 353.16s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  54.77925992012024\n",
            "step :  20500 iters_100_time :  79.59614515304565\n",
            "step :  20600 iters_100_time :  79.66858386993408\n",
            "step :  20700 iters_100_time :  79.84613394737244\n",
            "[507/600][442]\n",
            "                    Loss_D: 1.24 Loss_G: 79.48 Time: 352.80s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  21.960842609405518\n",
            "step :  20900 iters_100_time :  79.51232314109802\n",
            "step :  21000 iters_100_time :  79.61795711517334\n",
            "step :  21100 iters_100_time :  79.7916030883789\n",
            "step :  21200 iters_100_time :  79.51545023918152\n",
            "[508/600][442]\n",
            "                    Loss_D: 0.56 Loss_G: 57.06 Time: 353.85s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  67.15163230895996\n",
            "step :  21400 iters_100_time :  80.01203942298889\n",
            "step :  21500 iters_100_time :  79.1662654876709\n",
            "step :  21600 iters_100_time :  79.39540505409241\n",
            "[509/600][442]\n",
            "                    Loss_D: 2.05 Loss_G: 58.71 Time: 352.35s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  34.581809759140015\n",
            "step :  21800 iters_100_time :  79.58726072311401\n",
            "step :  21900 iters_100_time :  79.84392762184143\n",
            "step :  22000 iters_100_time :  79.97516059875488\n",
            "step :  22100 iters_100_time :  78.85945653915405\n",
            "[510/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 70.94 Time: 353.20s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  80.49239611625671\n",
            "step :  22300 iters_100_time :  79.96988987922668\n",
            "step :  22400 iters_100_time :  79.86960864067078\n",
            "step :  22500 iters_100_time :  79.44697546958923\n",
            "[511/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 75.29 Time: 353.50s\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  47.0715970993042\n",
            "step :  22700 iters_100_time :  79.6486165523529\n",
            "step :  22800 iters_100_time :  79.26887965202332\n",
            "step :  22900 iters_100_time :  79.48502564430237\n",
            "[512/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 50.54 Time: 352.41s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  13.965494155883789\n",
            "step :  23100 iters_100_time :  80.5875015258789\n",
            "step :  23200 iters_100_time :  79.89837265014648\n",
            "step :  23300 iters_100_time :  78.82489395141602\n",
            "step :  23400 iters_100_time :  79.52386522293091\n",
            "[513/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 78.85 Time: 354.43s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  59.62107014656067\n",
            "step :  23600 iters_100_time :  79.25138926506042\n",
            "step :  23700 iters_100_time :  80.0325334072113\n",
            "step :  23800 iters_100_time :  79.5253279209137\n",
            "[514/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 65.90 Time: 352.73s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  26.177301168441772\n",
            "step :  24000 iters_100_time :  79.61843037605286\n",
            "step :  24100 iters_100_time :  80.1249725818634\n",
            "step :  24200 iters_100_time :  78.73950862884521\n",
            "step :  24300 iters_100_time :  79.44059753417969\n",
            "[515/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 55.46 Time: 352.40s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  72.60788249969482\n",
            "step :  24500 iters_100_time :  79.65858101844788\n",
            "step :  24600 iters_100_time :  79.94112634658813\n",
            "step :  24700 iters_100_time :  79.57480549812317\n",
            "[516/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 59.07 Time: 353.78s\n",
            "num_batches :  442\n",
            "step :  24800 iters_100_time :  38.80147409439087\n",
            "step :  24900 iters_100_time :  79.61777901649475\n",
            "step :  25000 iters_100_time :  80.11421203613281\n",
            "step :  25000 iters_5000_time :  198.53359627723694\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.670939207077026\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  28.683146715164185\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  221.06654238700867\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  25100 iters_100_time :  360.87209129333496\n",
            "[517/600][442]\n",
            "                    Loss_D: 0.38 Loss_G: 56.45 Time: 635.80s\n",
            "num_batches :  442\n",
            "step :  25200 iters_100_time :  5.384638071060181\n",
            "step :  25300 iters_100_time :  80.45071291923523\n",
            "step :  25400 iters_100_time :  80.09936952590942\n",
            "step :  25500 iters_100_time :  80.11537766456604\n",
            "step :  25600 iters_100_time :  79.86444187164307\n",
            "[518/600][442]\n",
            "                    Loss_D: 1.95 Loss_G: 55.70 Time: 355.30s\n",
            "num_batches :  442\n",
            "step :  25700 iters_100_time :  52.15314435958862\n",
            "step :  25800 iters_100_time :  79.96384620666504\n",
            "step :  25900 iters_100_time :  79.93031787872314\n",
            "step :  26000 iters_100_time :  80.22437024116516\n",
            "[519/600][442]\n",
            "                    Loss_D: 0.69 Loss_G: 83.02 Time: 355.48s\n",
            "num_batches :  442\n",
            "step :  26100 iters_100_time :  18.14563512802124\n",
            "step :  26200 iters_100_time :  80.03696870803833\n",
            "step :  26300 iters_100_time :  80.35216164588928\n",
            "step :  26400 iters_100_time :  80.416175365448\n",
            "step :  26500 iters_100_time :  80.45652484893799\n",
            "[520/600][442]\n",
            "                    Loss_D: 0.89 Loss_G: 56.96 Time: 355.93s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  26600 iters_100_time :  65.56464099884033\n",
            "step :  26700 iters_100_time :  79.63451504707336\n",
            "step :  26800 iters_100_time :  79.81358528137207\n",
            "step :  26900 iters_100_time :  79.99734616279602\n",
            "[521/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 54.13 Time: 355.05s\n",
            "num_batches :  442\n",
            "step :  27000 iters_100_time :  30.96713161468506\n",
            "step :  27100 iters_100_time :  79.9450409412384\n",
            "step :  27200 iters_100_time :  79.42735981941223\n",
            "step :  27300 iters_100_time :  79.76463484764099\n",
            "step :  27400 iters_100_time :  79.83176684379578\n",
            "[522/600][442]\n",
            "                    Loss_D: 0.18 Loss_G: 57.05 Time: 354.03s\n",
            "num_batches :  442\n",
            "step :  27500 iters_100_time :  77.4357283115387\n",
            "step :  27600 iters_100_time :  79.44376564025879\n",
            "step :  27700 iters_100_time :  80.11814832687378\n",
            "step :  27800 iters_100_time :  79.23977470397949\n",
            "[523/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 67.64 Time: 353.19s\n",
            "num_batches :  442\n",
            "step :  27900 iters_100_time :  43.71947193145752\n",
            "step :  28000 iters_100_time :  79.25976610183716\n",
            "step :  28100 iters_100_time :  79.86519265174866\n",
            "step :  28200 iters_100_time :  79.67572093009949\n",
            "[524/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 54.51 Time: 353.35s\n",
            "num_batches :  442\n",
            "step :  28300 iters_100_time :  10.09005355834961\n",
            "step :  28400 iters_100_time :  79.44731616973877\n",
            "step :  28500 iters_100_time :  79.73383116722107\n",
            "step :  28600 iters_100_time :  79.79700255393982\n",
            "step :  28700 iters_100_time :  79.618577003479\n",
            "[525/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 52.01 Time: 352.70s\n",
            "num_batches :  442\n",
            "step :  28800 iters_100_time :  56.6608829498291\n",
            "step :  28900 iters_100_time :  79.47656631469727\n",
            "step :  29000 iters_100_time :  79.93227863311768\n",
            "step :  29100 iters_100_time :  79.71448469161987\n",
            "[526/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 72.17 Time: 353.24s\n",
            "num_batches :  442\n",
            "step :  29200 iters_100_time :  23.310855627059937\n",
            "step :  29300 iters_100_time :  79.81649422645569\n",
            "step :  29400 iters_100_time :  79.37836170196533\n",
            "step :  29500 iters_100_time :  79.17642998695374\n",
            "step :  29600 iters_100_time :  79.96842765808105\n",
            "[527/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 70.38 Time: 353.01s\n",
            "num_batches :  442\n",
            "step :  29700 iters_100_time :  69.59576344490051\n",
            "step :  29800 iters_100_time :  79.32798194885254\n",
            "step :  29900 iters_100_time :  80.17377519607544\n",
            "step :  30000 iters_100_time :  79.49473738670349\n",
            "[528/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 55.23 Time: 353.45s\n",
            "num_batches :  442\n",
            "step :  30100 iters_100_time :  36.20832324028015\n",
            "step :  30200 iters_100_time :  79.14315414428711\n",
            "step :  30300 iters_100_time :  79.53873896598816\n",
            "step :  30400 iters_100_time :  80.220712184906\n",
            "[529/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 68.99 Time: 353.61s\n",
            "num_batches :  442\n",
            "step :  30500 iters_100_time :  2.1930923461914062\n",
            "step :  30600 iters_100_time :  79.74177074432373\n",
            "step :  30700 iters_100_time :  79.65965747833252\n",
            "step :  30800 iters_100_time :  80.00435423851013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBwYuwliPq-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}