{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ef10d99e-c412-4107-83a8-a58f05db1981"
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'bird/': No such file or directory\n",
            "Cloning into 'CUB-Attn-GAN'...\n",
            "remote: Enumerating objects: 292, done.\u001b[K\n",
            "remote: Counting objects: 100% (292/292), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 842 (delta 197), reused 193 (delta 103), pack-reused 550\u001b[K\n",
            "Receiving objects: 100% (842/842), 475.00 MiB | 39.85 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "Checking out files: 100% (117/117), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "fe43a376-7ef3-4519-e1f1-3189f03e6d7e"
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-14 21:26:02--  https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.199.102, 74.125.199.138, 74.125.199.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.199.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8cp4onueoufk1hhnncfhkascas07bsia/1597440300000/17309505201871794426/*/1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-14 21:26:03--  https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8cp4onueoufk1hhnncfhkascas07bsia/1597440300000/17309505201871794426/*/1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111?e=download\n",
            "Resolving doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257793 (252K) [application/x-rar]\n",
            "Saving to: ‘Pillow.rar’\n",
            "\n",
            "Pillow.rar          100%[===================>] 251.75K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-08-14 21:26:03 (76.7 MB/s) - ‘Pillow.rar’ saved [257793/257793]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Pillow.rar\n",
            "\n",
            "Creating    Pillow                                                    OK\n",
            "Creating    Pillow/Tests                                              OK\n",
            "Creating    Pillow/Tests/fonts                                        OK\n",
            "Extracting  Pillow/Tests/fonts/FreeMono.ttf                              \b\b\b\b 12%\b\b\b\b 25%\b\b\b\b 38%\b\b\b\b 50%\b\b\b\b 63%\b\b\b\b 76%\b\b\b\b 88%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_460.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7f34412d-344e-4fe2-ff80-d1d9e9e5576d"
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'SentLevelGANLoss/': No such file or directory\n",
            "Cloning into 'SentLevelGANLoss'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 108 (delta 44), reused 94 (delta 33), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (108/108), 867.50 KiB | 19.72 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64769019-ce65-4d2b-aa75-dcc30ecfd26b"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_400.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:03<00:00, 28.5MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_400.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS =========  401\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  79.49513483047485\n",
            "step :  200 iters_100_time :  75.23300909996033\n",
            "step :  300 iters_100_time :  75.87799572944641\n",
            "step :  400 iters_100_time :  75.7905809879303\n",
            "[401/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 46.84 Time: 338.38s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  44.77981972694397\n",
            "step :  600 iters_100_time :  75.45682382583618\n",
            "step :  700 iters_100_time :  75.85798954963684\n",
            "step :  800 iters_100_time :  76.07193088531494\n",
            "[402/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 61.27 Time: 335.20s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  12.696837663650513\n",
            "step :  1000 iters_100_time :  75.55386781692505\n",
            "step :  1100 iters_100_time :  75.81328344345093\n",
            "step :  1200 iters_100_time :  75.5626814365387\n",
            "step :  1300 iters_100_time :  76.24414610862732\n",
            "[403/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 71.82 Time: 335.78s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  56.636335134506226\n",
            "step :  1500 iters_100_time :  75.90105366706848\n",
            "step :  1600 iters_100_time :  76.0233633518219\n",
            "step :  1700 iters_100_time :  75.98340320587158\n",
            "[404/600][442]\n",
            "                    Loss_D: 0.42 Loss_G: 64.06 Time: 336.43s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  25.297672986984253\n",
            "step :  1900 iters_100_time :  75.7975971698761\n",
            "step :  2000 iters_100_time :  75.83631610870361\n",
            "step :  2100 iters_100_time :  75.6514093875885\n",
            "step :  2200 iters_100_time :  76.27426838874817\n",
            "[405/600][442]\n",
            "                    Loss_D: 0.71 Loss_G: 67.88 Time: 336.49s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  69.12493371963501\n",
            "step :  2400 iters_100_time :  75.58386421203613\n",
            "step :  2500 iters_100_time :  76.03833889961243\n",
            "step :  2600 iters_100_time :  75.90735030174255\n",
            "[406/600][442]\n",
            "                    Loss_D: 0.23 Loss_G: 55.81 Time: 336.42s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  37.28399038314819\n",
            "step :  2800 iters_100_time :  75.68536186218262\n",
            "step :  2900 iters_100_time :  75.87678503990173\n",
            "step :  3000 iters_100_time :  75.71743178367615\n",
            "[407/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 51.32 Time: 336.06s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  5.308525800704956\n",
            "step :  3200 iters_100_time :  75.59166407585144\n",
            "step :  3300 iters_100_time :  75.58711242675781\n",
            "step :  3400 iters_100_time :  75.89629411697388\n",
            "step :  3500 iters_100_time :  75.611172914505\n",
            "[408/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 51.35 Time: 335.93s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  49.18410563468933\n",
            "step :  3700 iters_100_time :  75.91219758987427\n",
            "step :  3800 iters_100_time :  75.89897608757019\n",
            "step :  3900 iters_100_time :  75.71808123588562\n",
            "[409/600][442]\n",
            "                    Loss_D: 2.78 Loss_G: 42.31 Time: 336.30s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  17.272485733032227\n",
            "step :  4100 iters_100_time :  75.618492603302\n",
            "step :  4200 iters_100_time :  76.07514786720276\n",
            "step :  4300 iters_100_time :  75.80558586120605\n",
            "step :  4400 iters_100_time :  75.97342801094055\n",
            "[410/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 52.19 Time: 336.14s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  61.91905999183655\n",
            "step :  4600 iters_100_time :  75.85382771492004\n",
            "step :  4700 iters_100_time :  75.57357358932495\n",
            "step :  4800 iters_100_time :  75.88917517662048\n",
            "[411/600][442]\n",
            "                    Loss_D: 0.59 Loss_G: 64.30 Time: 336.79s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  29.540722608566284\n",
            "step :  5000 iters_100_time :  75.82708168029785\n",
            "step :  5100 iters_100_time :  75.93526482582092\n",
            "step :  5200 iters_100_time :  75.72925329208374\n",
            "step :  5300 iters_100_time :  75.61359095573425\n",
            "[412/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 56.75 Time: 336.39s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  73.32914590835571\n",
            "step :  5500 iters_100_time :  76.24558854103088\n",
            "step :  5600 iters_100_time :  75.78564691543579\n",
            "step :  5700 iters_100_time :  75.98500561714172\n",
            "[413/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 53.23 Time: 336.79s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  41.50182008743286\n",
            "step :  5900 iters_100_time :  76.11875128746033\n",
            "step :  6000 iters_100_time :  76.3494963645935\n",
            "step :  6100 iters_100_time :  75.88703179359436\n",
            "[414/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 63.34 Time: 337.37s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  9.544625520706177\n",
            "step :  6300 iters_100_time :  76.15266060829163\n",
            "step :  6400 iters_100_time :  75.50722050666809\n",
            "step :  6500 iters_100_time :  76.4948046207428\n",
            "step :  6600 iters_100_time :  75.78571939468384\n",
            "[415/600][442]\n",
            "                    Loss_D: 0.44 Loss_G: 57.51 Time: 337.10s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  53.93013668060303\n",
            "step :  6800 iters_100_time :  75.89279842376709\n",
            "step :  6900 iters_100_time :  75.63110256195068\n",
            "step :  7000 iters_100_time :  76.458575963974\n",
            "[416/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 45.98 Time: 336.96s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  22.191592693328857\n",
            "step :  7200 iters_100_time :  76.0363097190857\n",
            "step :  7300 iters_100_time :  76.0078432559967\n",
            "step :  7400 iters_100_time :  76.25878596305847\n",
            "step :  7500 iters_100_time :  76.12158203125\n",
            "[417/600][442]\n",
            "                    Loss_D: 0.37 Loss_G: 59.00 Time: 337.53s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  65.9955952167511\n",
            "step :  7700 iters_100_time :  76.11369395256042\n",
            "step :  7800 iters_100_time :  76.29871106147766\n",
            "step :  7900 iters_100_time :  76.36292958259583\n",
            "[418/600][442]\n",
            "                    Loss_D: 0.23 Loss_G: 57.82 Time: 337.56s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  34.33658051490784\n",
            "step :  8100 iters_100_time :  76.01718282699585\n",
            "step :  8200 iters_100_time :  75.59788966178894\n",
            "step :  8300 iters_100_time :  76.10464286804199\n",
            "[419/600][442]\n",
            "                    Loss_D: 0.61 Loss_G: 65.91 Time: 336.49s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.4292407035827637\n",
            "step :  8500 iters_100_time :  75.64110732078552\n",
            "step :  8600 iters_100_time :  76.0733642578125\n",
            "step :  8700 iters_100_time :  75.77299308776855\n",
            "step :  8800 iters_100_time :  76.33875513076782\n",
            "[420/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 63.81 Time: 337.22s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  46.41616892814636\n",
            "step :  9000 iters_100_time :  76.05649471282959\n",
            "step :  9100 iters_100_time :  76.33221364021301\n",
            "step :  9200 iters_100_time :  75.89405870437622\n",
            "[421/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 56.61 Time: 337.60s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  14.81950306892395\n",
            "step :  9400 iters_100_time :  76.10793209075928\n",
            "step :  9500 iters_100_time :  76.27639484405518\n",
            "step :  9600 iters_100_time :  76.00762605667114\n",
            "step :  9700 iters_100_time :  76.83009195327759\n",
            "[422/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 51.12 Time: 338.18s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  59.24024796485901\n",
            "step :  9900 iters_100_time :  76.21140909194946\n",
            "step :  10000 iters_100_time :  76.43911409378052\n",
            "step :  10100 iters_100_time :  76.36319375038147\n",
            "[423/600][442]\n",
            "                    Loss_D: 1.12 Loss_G: 37.65 Time: 338.95s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  26.824411630630493\n",
            "step :  10300 iters_100_time :  76.5151994228363\n",
            "step :  10400 iters_100_time :  76.6574170589447\n",
            "step :  10500 iters_100_time :  76.24174237251282\n",
            "step :  10600 iters_100_time :  76.1733455657959\n",
            "[424/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 56.33 Time: 338.80s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  71.11710500717163\n",
            "step :  10800 iters_100_time :  76.29515838623047\n",
            "step :  10900 iters_100_time :  76.58991503715515\n",
            "step :  11000 iters_100_time :  76.27918791770935\n",
            "[425/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 73.06 Time: 338.98s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  39.15890836715698\n",
            "step :  11200 iters_100_time :  76.3490617275238\n",
            "step :  11300 iters_100_time :  76.28004264831543\n",
            "step :  11400 iters_100_time :  76.46828174591064\n",
            "[426/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 56.14 Time: 338.69s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  6.786381244659424\n",
            "step :  11600 iters_100_time :  76.43175983428955\n",
            "step :  11700 iters_100_time :  76.85462737083435\n",
            "step :  11800 iters_100_time :  76.1918318271637\n",
            "step :  11900 iters_100_time :  76.67173600196838\n",
            "[427/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 70.38 Time: 339.20s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  51.27916240692139\n",
            "step :  12100 iters_100_time :  76.20332169532776\n",
            "step :  12200 iters_100_time :  77.35115838050842\n",
            "step :  12300 iters_100_time :  76.07673573493958\n",
            "[428/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 71.23 Time: 339.66s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  18.909387826919556\n",
            "step :  12500 iters_100_time :  76.24726605415344\n",
            "step :  12600 iters_100_time :  76.88857054710388\n",
            "step :  12700 iters_100_time :  76.14281868934631\n",
            "step :  12800 iters_100_time :  76.59239196777344\n",
            "[429/600][442]\n",
            "                    Loss_D: 0.37 Loss_G: 68.47 Time: 338.81s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  63.33098602294922\n",
            "step :  13000 iters_100_time :  76.54819989204407\n",
            "step :  13100 iters_100_time :  76.92251753807068\n",
            "step :  13200 iters_100_time :  76.49619555473328\n",
            "[430/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 59.66 Time: 339.16s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  31.40472149848938\n",
            "step :  13400 iters_100_time :  77.25699234008789\n",
            "step :  13500 iters_100_time :  77.02363157272339\n",
            "step :  13600 iters_100_time :  76.5767469406128\n",
            "step :  13700 iters_100_time :  76.51658725738525\n",
            "[431/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 54.86 Time: 340.98s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  76.00567555427551\n",
            "step :  13900 iters_100_time :  76.944340467453\n",
            "step :  14000 iters_100_time :  75.93610978126526\n",
            "step :  14100 iters_100_time :  76.40312600135803\n",
            "[432/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 78.89 Time: 339.91s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  43.810356855392456\n",
            "step :  14300 iters_100_time :  76.60917067527771\n",
            "step :  14400 iters_100_time :  76.75914001464844\n",
            "step :  14500 iters_100_time :  77.77221155166626\n",
            "[433/600][442]\n",
            "                    Loss_D: 0.46 Loss_G: 63.30 Time: 341.20s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  11.791717767715454\n",
            "step :  14700 iters_100_time :  75.65852093696594\n",
            "step :  14800 iters_100_time :  76.79542708396912\n",
            "step :  14900 iters_100_time :  77.07283163070679\n",
            "step :  15000 iters_100_time :  77.56503081321716\n",
            "[434/600][442]\n",
            "                    Loss_D: 0.52 Loss_G: 61.91 Time: 340.66s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  55.202359676361084\n",
            "step :  15200 iters_100_time :  77.54624938964844\n",
            "step :  15300 iters_100_time :  77.47422671318054\n",
            "step :  15400 iters_100_time :  75.97189211845398\n",
            "[435/600][442]\n",
            "                    Loss_D: 1.31 Loss_G: 62.65 Time: 339.80s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  23.814829349517822\n",
            "step :  15600 iters_100_time :  77.43771314620972\n",
            "step :  15700 iters_100_time :  76.83569025993347\n",
            "step :  15800 iters_100_time :  77.48275589942932\n",
            "step :  15900 iters_100_time :  77.51861262321472\n",
            "[436/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 52.42 Time: 342.31s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  68.04147553443909\n",
            "step :  16100 iters_100_time :  76.27050614356995\n",
            "step :  16200 iters_100_time :  77.48654842376709\n",
            "step :  16300 iters_100_time :  77.31387853622437\n",
            "[437/600][442]\n",
            "                    Loss_D: 0.28 Loss_G: 49.89 Time: 340.97s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  35.51585793495178\n",
            "step :  16500 iters_100_time :  77.46300315856934\n",
            "step :  16600 iters_100_time :  77.69083762168884\n",
            "step :  16700 iters_100_time :  76.24229407310486\n",
            "[438/600][442]\n",
            "                    Loss_D: 0.54 Loss_G: 69.51 Time: 340.56s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  3.459041118621826\n",
            "step :  16900 iters_100_time :  77.2451536655426\n",
            "step :  17000 iters_100_time :  77.42243671417236\n",
            "step :  17100 iters_100_time :  76.37796998023987\n",
            "step :  17200 iters_100_time :  76.7244393825531\n",
            "[439/600][442]\n",
            "                    Loss_D: 0.21 Loss_G: 44.64 Time: 341.09s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  48.96814560890198\n",
            "step :  17400 iters_100_time :  76.20430374145508\n",
            "step :  17500 iters_100_time :  76.44895720481873\n",
            "step :  17600 iters_100_time :  77.29377436637878\n",
            "[440/600][442]\n",
            "                    Loss_D: 0.17 Loss_G: 49.60 Time: 341.07s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  15.95863389968872\n",
            "step :  17800 iters_100_time :  76.48557829856873\n",
            "step :  17900 iters_100_time :  76.15771865844727\n",
            "step :  18000 iters_100_time :  77.31569814682007\n",
            "step :  18100 iters_100_time :  77.51234102249146\n",
            "[441/600][442]\n",
            "                    Loss_D: 1.26 Loss_G: 75.70 Time: 340.89s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  60.773009061813354\n",
            "step :  18300 iters_100_time :  77.48602104187012\n",
            "step :  18400 iters_100_time :  77.08229112625122\n",
            "step :  18500 iters_100_time :  77.01195335388184\n",
            "[442/600][442]\n",
            "                    Loss_D: 0.29 Loss_G: 70.82 Time: 342.07s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  28.22849678993225\n",
            "step :  18700 iters_100_time :  76.48406934738159\n",
            "step :  18800 iters_100_time :  77.29219150543213\n",
            "step :  18900 iters_100_time :  76.37729620933533\n",
            "step :  19000 iters_100_time :  76.64809346199036\n",
            "[443/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 50.51 Time: 339.94s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  73.49294972419739\n",
            "step :  19200 iters_100_time :  77.108234167099\n",
            "step :  19300 iters_100_time :  77.00602197647095\n",
            "step :  19400 iters_100_time :  77.27637600898743\n",
            "[444/600][442]\n",
            "                    Loss_D: 1.21 Loss_G: 75.86 Time: 342.27s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  40.73735451698303\n",
            "step :  19600 iters_100_time :  76.28531384468079\n",
            "step :  19700 iters_100_time :  77.34498572349548\n",
            "step :  19800 iters_100_time :  76.14457488059998\n",
            "[445/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 46.39 Time: 339.53s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  8.10185980796814\n",
            "step :  20000 iters_100_time :  77.26597285270691\n",
            "step :  20100 iters_100_time :  77.25853681564331\n",
            "step :  20200 iters_100_time :  77.03114771842957\n",
            "step :  20300 iters_100_time :  77.25662469863892\n",
            "[446/600][442]\n",
            "                    Loss_D: 0.27 Loss_G: 63.52 Time: 342.12s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  52.87436270713806\n",
            "step :  20500 iters_100_time :  76.8771722316742\n",
            "step :  20600 iters_100_time :  76.86998748779297\n",
            "step :  20700 iters_100_time :  76.26905131340027\n",
            "[447/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 62.89 Time: 340.14s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  20.535906553268433\n",
            "step :  20900 iters_100_time :  77.63132548332214\n",
            "step :  21000 iters_100_time :  75.52323055267334\n",
            "step :  21100 iters_100_time :  76.89399075508118\n",
            "step :  21200 iters_100_time :  77.6493182182312\n",
            "[448/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 63.56 Time: 340.73s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  64.70157170295715\n",
            "step :  21400 iters_100_time :  76.68962597846985\n",
            "step :  21500 iters_100_time :  77.1923942565918\n",
            "step :  21600 iters_100_time :  77.72138404846191\n",
            "[449/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 61.81 Time: 341.25s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  33.046974182128906\n",
            "step :  21800 iters_100_time :  77.71437406539917\n",
            "step :  21900 iters_100_time :  76.25073623657227\n",
            "step :  22000 iters_100_time :  76.33972382545471\n",
            "step :  22100 iters_100_time :  77.3960485458374\n",
            "[450/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 57.53 Time: 341.12s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  77.6644389629364\n",
            "step :  22300 iters_100_time :  77.35873556137085\n",
            "step :  22400 iters_100_time :  77.35206604003906\n",
            "step :  22500 iters_100_time :  76.69210815429688\n",
            "[451/600][442]\n",
            "                    Loss_D: 0.10 Loss_G: 76.35 Time: 341.81s\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  45.40532207489014\n",
            "step :  22700 iters_100_time :  77.41992807388306\n",
            "step :  22800 iters_100_time :  76.99223113059998\n",
            "step :  22900 iters_100_time :  77.2460069656372\n",
            "[452/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 57.71 Time: 342.42s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  13.183370113372803\n",
            "step :  23100 iters_100_time :  76.79533505439758\n",
            "step :  23200 iters_100_time :  77.61031770706177\n",
            "step :  23300 iters_100_time :  77.5081615447998\n",
            "step :  23400 iters_100_time :  76.46939373016357\n",
            "[453/600][442]\n",
            "                    Loss_D: 0.23 Loss_G: 69.93 Time: 341.25s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  58.34604024887085\n",
            "step :  23600 iters_100_time :  77.37891292572021\n",
            "step :  23700 iters_100_time :  77.09167766571045\n",
            "step :  23800 iters_100_time :  77.59804773330688\n",
            "[454/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 66.32 Time: 343.66s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  25.54397702217102\n",
            "step :  24000 iters_100_time :  76.83887147903442\n",
            "step :  24100 iters_100_time :  77.57038402557373\n",
            "step :  24200 iters_100_time :  77.20492720603943\n",
            "step :  24300 iters_100_time :  76.91498756408691\n",
            "[455/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 46.95 Time: 341.91s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  70.69127678871155\n",
            "step :  24500 iters_100_time :  77.48123359680176\n",
            "step :  24600 iters_100_time :  77.40269994735718\n",
            "step :  24700 iters_100_time :  77.51349663734436\n",
            "[456/600][442]\n",
            "                    Loss_D: 0.18 Loss_G: 63.38 Time: 344.09s\n",
            "num_batches :  442\n",
            "step :  24800 iters_100_time :  38.047109603881836\n",
            "step :  24900 iters_100_time :  77.11235618591309\n",
            "step :  25000 iters_100_time :  77.76839685440063\n",
            "step :  25000 iters_5000_time :  192.927996635437\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  6.939862966537476\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.37994360923767\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  192.91087889671326\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  25100 iters_100_time :  323.28903365135193\n",
            "[457/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 57.17 Time: 589.52s\n",
            "num_batches :  442\n",
            "step :  25200 iters_100_time :  5.386881351470947\n",
            "step :  25300 iters_100_time :  77.81823754310608\n",
            "step :  25400 iters_100_time :  76.85079979896545\n",
            "step :  25500 iters_100_time :  76.5400116443634\n",
            "step :  25600 iters_100_time :  77.84382438659668\n",
            "[458/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 68.16 Time: 343.15s\n",
            "num_batches :  442\n",
            "step :  25700 iters_100_time :  50.179919958114624\n",
            "step :  25800 iters_100_time :  77.60789346694946\n",
            "step :  25900 iters_100_time :  77.95447134971619\n",
            "step :  26000 iters_100_time :  76.8508141040802\n",
            "[459/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 57.04 Time: 342.85s\n",
            "num_batches :  442\n",
            "step :  26100 iters_100_time :  17.852120399475098\n",
            "step :  26200 iters_100_time :  78.0812509059906\n",
            "step :  26300 iters_100_time :  79.35578393936157\n",
            "step :  26400 iters_100_time :  78.92706060409546\n",
            "step :  26500 iters_100_time :  78.79365730285645\n",
            "[460/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 61.35 Time: 349.70s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  26600 iters_100_time :  63.31702947616577\n",
            "step :  26700 iters_100_time :  77.28794527053833\n",
            "step :  26800 iters_100_time :  77.30303502082825\n",
            "step :  26900 iters_100_time :  77.6312506198883\n",
            "[461/600][442]\n",
            "                    Loss_D: 0.59 Loss_G: 63.36 Time: 343.96s\n",
            "num_batches :  442\n",
            "step :  27000 iters_100_time :  29.758410215377808\n",
            "step :  27100 iters_100_time :  78.04996919631958\n",
            "step :  27200 iters_100_time :  77.93596744537354\n",
            "step :  27300 iters_100_time :  77.04427099227905\n",
            "step :  27400 iters_100_time :  77.08047842979431\n",
            "[462/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 82.01 Time: 343.09s\n",
            "num_batches :  442\n",
            "step :  27500 iters_100_time :  75.72719240188599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBwYuwliPq-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}