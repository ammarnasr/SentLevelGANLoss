{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "1431a090-6740-49aa-b483-1a783990a521"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 10 06:33:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_110.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "04b91e5b-4b5c-4faf-f2d7-eeab62c31173"
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'SentLevelGANLoss/': No such file or directory\n",
            "Cloning into 'SentLevelGANLoss'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 54 (delta 16), reused 47 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c545a0f6-48b3-4586-a122-87c22f0fb34a"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_60.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:00<00:00, 281MB/s] \n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_60.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS =========  61\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  118.53577256202698\n",
            "step :  200 iters_100_time :  117.14036726951599\n",
            "step :  300 iters_100_time :  119.19539284706116\n",
            "step :  400 iters_100_time :  120.31293416023254\n",
            "[61/600][442]\n",
            "                    Loss_D: 0.36 Loss_G: 61.67 Time: 525.85s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  70.18959736824036\n",
            "step :  600 iters_100_time :  120.2280924320221\n",
            "step :  700 iters_100_time :  120.35822629928589\n",
            "step :  800 iters_100_time :  120.49041557312012\n",
            "[62/600][442]\n",
            "                    Loss_D: 2.04 Loss_G: 64.17 Time: 532.74s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  19.522716760635376\n",
            "step :  1000 iters_100_time :  120.13947916030884\n",
            "step :  1100 iters_100_time :  120.31198263168335\n",
            "step :  1200 iters_100_time :  120.40086317062378\n",
            "step :  1300 iters_100_time :  120.55167317390442\n",
            "[63/600][442]\n",
            "                    Loss_D: 1.90 Loss_G: 38.07 Time: 532.61s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  89.33343005180359\n",
            "step :  1500 iters_100_time :  120.1630744934082\n",
            "step :  1600 iters_100_time :  120.37002229690552\n",
            "step :  1700 iters_100_time :  120.43677711486816\n",
            "[64/600][442]\n",
            "                    Loss_D: 0.53 Loss_G: 46.45 Time: 532.75s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  38.82671761512756\n",
            "step :  1900 iters_100_time :  120.4839699268341\n",
            "step :  2000 iters_100_time :  120.34834671020508\n",
            "step :  2100 iters_100_time :  120.48759961128235\n",
            "step :  2200 iters_100_time :  120.55965495109558\n",
            "[65/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 49.53 Time: 533.14s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  108.7130868434906\n",
            "step :  2400 iters_100_time :  120.5913827419281\n",
            "step :  2500 iters_100_time :  120.56775116920471\n",
            "step :  2600 iters_100_time :  120.5520875453949\n",
            "[66/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 45.80 Time: 533.53s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  58.37583684921265\n",
            "step :  2800 iters_100_time :  120.5157482624054\n",
            "step :  2900 iters_100_time :  120.70200085639954\n",
            "step :  3000 iters_100_time :  120.70544576644897\n",
            "[67/600][442]\n",
            "                    Loss_D: 1.46 Loss_G: 54.85 Time: 534.17s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  7.673926115036011\n",
            "step :  3200 iters_100_time :  120.791579246521\n",
            "step :  3300 iters_100_time :  120.57172083854675\n",
            "step :  3400 iters_100_time :  120.6104941368103\n",
            "step :  3500 iters_100_time :  120.64066815376282\n",
            "[68/600][442]\n",
            "                    Loss_D: 1.11 Loss_G: 50.93 Time: 534.16s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  77.63439011573792\n",
            "step :  3700 iters_100_time :  120.94473624229431\n",
            "step :  3800 iters_100_time :  120.66056394577026\n",
            "step :  3900 iters_100_time :  120.5635392665863\n",
            "[69/600][442]\n",
            "                    Loss_D: 0.68 Loss_G: 53.62 Time: 534.30s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  26.886394262313843\n",
            "step :  4100 iters_100_time :  120.50368523597717\n",
            "step :  4200 iters_100_time :  120.79903960227966\n",
            "step :  4300 iters_100_time :  120.71545481681824\n",
            "step :  4400 iters_100_time :  120.63000559806824\n",
            "[70/600][442]\n",
            "                    Loss_D: 0.17 Loss_G: 40.62 Time: 534.03s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  96.88032388687134\n",
            "step :  4600 iters_100_time :  120.77499103546143\n",
            "step :  4700 iters_100_time :  120.4860029220581\n",
            "step :  4800 iters_100_time :  120.412832736969\n",
            "[71/600][442]\n",
            "                    Loss_D: 0.56 Loss_G: 38.71 Time: 533.81s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  46.11449313163757\n",
            "step :  5000 iters_100_time :  120.67939352989197\n",
            "step :  5100 iters_100_time :  120.56954169273376\n",
            "step :  5200 iters_100_time :  120.58906579017639\n",
            "step :  5300 iters_100_time :  120.61396908760071\n",
            "[72/600][442]\n",
            "                    Loss_D: 0.51 Loss_G: 43.94 Time: 533.91s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  116.10215783119202\n",
            "step :  5500 iters_100_time :  120.67715764045715\n",
            "step :  5600 iters_100_time :  120.69257926940918\n",
            "step :  5700 iters_100_time :  120.40117192268372\n",
            "[73/600][442]\n",
            "                    Loss_D: 1.91 Loss_G: 52.79 Time: 533.71s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  65.34902906417847\n",
            "step :  5900 iters_100_time :  120.6436858177185\n",
            "step :  6000 iters_100_time :  120.59254622459412\n",
            "step :  6100 iters_100_time :  120.58113241195679\n",
            "[74/600][442]\n",
            "                    Loss_D: 0.40 Loss_G: 48.14 Time: 533.73s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  14.87119460105896\n",
            "step :  6300 iters_100_time :  120.7433762550354\n",
            "step :  6400 iters_100_time :  120.54055380821228\n",
            "step :  6500 iters_100_time :  120.56699275970459\n",
            "step :  6600 iters_100_time :  120.34227061271667\n",
            "[75/600][442]\n",
            "                    Loss_D: 2.53 Loss_G: 55.46 Time: 533.60s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  84.78877902030945\n",
            "step :  6800 iters_100_time :  120.5589919090271\n",
            "step :  6900 iters_100_time :  120.43039035797119\n",
            "step :  7000 iters_100_time :  120.46022629737854\n",
            "[76/600][442]\n",
            "                    Loss_D: 0.18 Loss_G: 62.72 Time: 533.44s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  34.082306146621704\n",
            "step :  7200 iters_100_time :  120.55965042114258\n",
            "step :  7300 iters_100_time :  120.47817492485046\n",
            "step :  7400 iters_100_time :  120.45375442504883\n",
            "step :  7500 iters_100_time :  120.3905439376831\n",
            "[77/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 41.42 Time: 533.25s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  104.05795335769653\n",
            "step :  7700 iters_100_time :  120.33720970153809\n",
            "step :  7800 iters_100_time :  120.43247127532959\n",
            "step :  7900 iters_100_time :  120.42197632789612\n",
            "[78/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 59.36 Time: 533.10s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  53.429970502853394\n",
            "step :  8100 iters_100_time :  120.46859240531921\n",
            "step :  8200 iters_100_time :  120.51581406593323\n",
            "step :  8300 iters_100_time :  120.48017454147339\n",
            "[79/600][442]\n",
            "                    Loss_D: 0.38 Loss_G: 44.51 Time: 533.64s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.728022575378418\n",
            "step :  8500 iters_100_time :  120.67121601104736\n",
            "step :  8600 iters_100_time :  120.54440641403198\n",
            "step :  8700 iters_100_time :  120.47077369689941\n",
            "step :  8800 iters_100_time :  120.55332493782043\n",
            "[80/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 42.64 Time: 533.50s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  72.89194345474243\n",
            "step :  9000 iters_100_time :  120.73004698753357\n",
            "step :  9100 iters_100_time :  120.73045468330383\n",
            "step :  9200 iters_100_time :  120.80300736427307\n",
            "[81/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 44.15 Time: 534.44s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  22.180652141571045\n",
            "step :  9400 iters_100_time :  120.81695771217346\n",
            "step :  9500 iters_100_time :  120.89059114456177\n",
            "step :  9600 iters_100_time :  120.82312440872192\n",
            "step :  9700 iters_100_time :  120.73454093933105\n",
            "[82/600][442]\n",
            "                    Loss_D: 0.60 Loss_G: 53.32 Time: 534.88s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  92.12639689445496\n",
            "step :  9900 iters_100_time :  120.62427687644958\n",
            "step :  10000 iters_100_time :  120.80654668807983\n",
            "step :  10100 iters_100_time :  120.61695861816406\n",
            "[83/600][442]\n",
            "                    Loss_D: 1.00 Loss_G: 55.36 Time: 534.39s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  41.579431772232056\n",
            "step :  10300 iters_100_time :  120.56108450889587\n",
            "step :  10400 iters_100_time :  120.73417949676514\n",
            "step :  10500 iters_100_time :  120.54548716545105\n",
            "step :  10600 iters_100_time :  120.9012885093689\n",
            "[84/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 37.11 Time: 534.31s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  111.72580671310425\n",
            "step :  10800 iters_100_time :  120.65946245193481\n",
            "step :  10900 iters_100_time :  120.70018672943115\n",
            "step :  11000 iters_100_time :  120.82141494750977\n",
            "[85/600][442]\n",
            "                    Loss_D: 0.47 Loss_G: 41.37 Time: 534.74s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  60.943498611450195\n",
            "step :  11200 iters_100_time :  120.79789710044861\n",
            "step :  11300 iters_100_time :  120.74942898750305\n",
            "step :  11400 iters_100_time :  120.7603907585144\n",
            "[86/600][442]\n",
            "                    Loss_D: 1.57 Loss_G: 47.34 Time: 534.68s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  10.11764407157898\n",
            "step :  11600 iters_100_time :  121.00019693374634\n",
            "step :  11700 iters_100_time :  120.95682287216187\n",
            "step :  11800 iters_100_time :  120.84012007713318\n",
            "step :  11900 iters_100_time :  120.59646534919739\n",
            "[87/600][442]\n",
            "                    Loss_D: 0.27 Loss_G: 53.89 Time: 535.02s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  80.16249918937683\n",
            "step :  12100 iters_100_time :  120.80485272407532\n",
            "step :  12200 iters_100_time :  120.80789852142334\n",
            "step :  12300 iters_100_time :  120.35368084907532\n",
            "[88/600][442]\n",
            "                    Loss_D: 2.84 Loss_G: 63.05 Time: 534.03s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  29.32965922355652\n",
            "step :  12500 iters_100_time :  120.79722094535828\n",
            "step :  12600 iters_100_time :  120.52964901924133\n",
            "step :  12700 iters_100_time :  120.62918543815613\n",
            "step :  12800 iters_100_time :  120.73633289337158\n",
            "[89/600][442]\n",
            "                    Loss_D: 0.68 Loss_G: 48.42 Time: 534.03s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  99.36843204498291\n",
            "step :  13000 iters_100_time :  120.94845747947693\n",
            "step :  13100 iters_100_time :  120.59315228462219\n",
            "step :  13200 iters_100_time :  120.61761784553528\n",
            "[90/600][442]\n",
            "                    Loss_D: 0.39 Loss_G: 40.44 Time: 534.30s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  48.75641751289368\n",
            "step :  13400 iters_100_time :  120.55701422691345\n",
            "step :  13500 iters_100_time :  120.55676603317261\n",
            "step :  13600 iters_100_time :  120.36986017227173\n",
            "step :  13700 iters_100_time :  120.50984025001526\n",
            "[91/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 47.17 Time: 533.66s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  118.57633781433105\n",
            "step :  13900 iters_100_time :  120.52322483062744\n",
            "step :  14000 iters_100_time :  120.59136486053467\n",
            "step :  14100 iters_100_time :  120.62986445426941\n",
            "[92/600][442]\n",
            "                    Loss_D: 1.03 Loss_G: 53.96 Time: 533.81s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  68.01193332672119\n",
            "step :  14300 iters_100_time :  120.56798696517944\n",
            "step :  14400 iters_100_time :  120.76346182823181\n",
            "step :  14500 iters_100_time :  120.66413021087646\n",
            "[93/600][442]\n",
            "                    Loss_D: 0.98 Loss_G: 49.17 Time: 534.03s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  17.329972743988037\n",
            "step :  14700 iters_100_time :  120.66676759719849\n",
            "step :  14800 iters_100_time :  120.6872627735138\n",
            "step :  14900 iters_100_time :  120.71534037590027\n",
            "step :  15000 iters_100_time :  120.63089346885681\n",
            "[94/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 49.76 Time: 534.26s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  87.18944263458252\n",
            "step :  15200 iters_100_time :  120.8237578868866\n",
            "step :  15300 iters_100_time :  120.64327311515808\n",
            "step :  15400 iters_100_time :  120.74424147605896\n",
            "[95/600][442]\n",
            "                    Loss_D: 1.34 Loss_G: 43.53 Time: 534.22s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  36.48630666732788\n",
            "step :  15600 iters_100_time :  120.76093316078186\n",
            "step :  15700 iters_100_time :  120.46529197692871\n",
            "step :  15800 iters_100_time :  120.673091173172\n",
            "step :  15900 iters_100_time :  120.3319685459137\n",
            "[96/600][442]\n",
            "                    Loss_D: 1.68 Loss_G: 36.00 Time: 533.61s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  106.50236177444458\n",
            "step :  16100 iters_100_time :  120.43163537979126\n",
            "step :  16200 iters_100_time :  120.41672444343567\n",
            "step :  16300 iters_100_time :  120.49333596229553\n",
            "[97/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 44.63 Time: 533.25s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  55.733113527297974\n",
            "step :  16500 iters_100_time :  120.55735850334167\n",
            "step :  16600 iters_100_time :  120.41459846496582\n",
            "step :  16700 iters_100_time :  120.4853150844574\n",
            "[98/600][442]\n",
            "                    Loss_D: 1.40 Loss_G: 45.64 Time: 532.93s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  5.118664741516113\n",
            "step :  16900 iters_100_time :  120.16553664207458\n",
            "step :  17000 iters_100_time :  120.42758750915527\n",
            "step :  17100 iters_100_time :  120.34892916679382\n",
            "step :  17200 iters_100_time :  120.41211175918579\n",
            "[99/600][442]\n",
            "                    Loss_D: 1.24 Loss_G: 44.23 Time: 532.64s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  75.0691397190094\n",
            "step :  17400 iters_100_time :  120.26752281188965\n",
            "step :  17500 iters_100_time :  120.46658539772034\n",
            "step :  17600 iters_100_time :  120.68464612960815\n",
            "[100/600][442]\n",
            "                    Loss_D: 0.84 Loss_G: 48.43 Time: 533.46s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  24.3498432636261\n",
            "step :  17800 iters_100_time :  120.44536447525024\n",
            "step :  17900 iters_100_time :  120.55599069595337\n",
            "step :  18000 iters_100_time :  120.67375946044922\n",
            "step :  18100 iters_100_time :  120.95406913757324\n",
            "[101/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 50.60 Time: 534.11s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  94.75178003311157\n",
            "step :  18300 iters_100_time :  120.68645787239075\n",
            "step :  18400 iters_100_time :  120.7330572605133\n",
            "step :  18500 iters_100_time :  120.76030945777893\n",
            "[102/600][442]\n",
            "                    Loss_D: 0.83 Loss_G: 37.62 Time: 534.53s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  43.92465138435364\n",
            "step :  18700 iters_100_time :  120.45306086540222\n",
            "step :  18800 iters_100_time :  120.50765037536621\n",
            "step :  18900 iters_100_time :  120.7188630104065\n",
            "step :  19000 iters_100_time :  120.51200652122498\n",
            "[103/600][442]\n",
            "                    Loss_D: 0.87 Loss_G: 39.93 Time: 533.78s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  114.03859424591064\n",
            "step :  19200 iters_100_time :  120.71059823036194\n",
            "step :  19300 iters_100_time :  120.60299348831177\n",
            "step :  19400 iters_100_time :  120.45522904396057\n",
            "[104/600][442]\n",
            "                    Loss_D: 1.39 Loss_G: 50.29 Time: 534.09s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  63.11915111541748\n",
            "step :  19600 iters_100_time :  120.57195162773132\n",
            "step :  19700 iters_100_time :  120.5126793384552\n",
            "step :  19800 iters_100_time :  120.47129678726196\n",
            "[105/600][442]\n",
            "                    Loss_D: 1.28 Loss_G: 59.73 Time: 533.73s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  12.516605138778687\n",
            "step :  20000 iters_100_time :  120.93269538879395\n",
            "step :  20100 iters_100_time :  120.62442326545715\n",
            "step :  20200 iters_100_time :  120.7416079044342\n",
            "step :  20300 iters_100_time :  120.6770327091217\n",
            "[106/600][442]\n",
            "                    Loss_D: 0.72 Loss_G: 37.69 Time: 534.49s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  82.46589159965515\n",
            "step :  20500 iters_100_time :  120.7772204875946\n",
            "step :  20600 iters_100_time :  120.59547543525696\n",
            "step :  20700 iters_100_time :  120.5890691280365\n",
            "[107/600][442]\n",
            "                    Loss_D: 0.26 Loss_G: 50.86 Time: 534.30s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  31.849750518798828\n",
            "step :  20900 iters_100_time :  120.71250772476196\n",
            "step :  21000 iters_100_time :  120.65649461746216\n",
            "step :  21100 iters_100_time :  120.97049522399902\n",
            "step :  21200 iters_100_time :  120.82531094551086\n",
            "[108/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 42.28 Time: 534.83s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  101.82515621185303\n",
            "step :  21400 iters_100_time :  120.76364588737488\n",
            "step :  21500 iters_100_time :  120.68851613998413\n",
            "step :  21600 iters_100_time :  120.50832796096802\n",
            "[109/600][442]\n",
            "                    Loss_D: 0.55 Loss_G: 41.97 Time: 534.04s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  51.1187629699707\n",
            "step :  21800 iters_100_time :  120.7183198928833\n",
            "step :  21900 iters_100_time :  120.63660001754761\n",
            "step :  22000 iters_100_time :  120.69729280471802\n",
            "step :  22100 iters_100_time :  120.65753197669983\n",
            "[110/600][442]\n",
            "                    Loss_D: 0.97 Loss_G: 48.05 Time: 534.26s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  121.1547417640686\n",
            "step :  22300 iters_100_time :  120.87854886054993\n",
            "step :  22400 iters_100_time :  120.60980749130249\n",
            "step :  22500 iters_100_time :  120.7432267665863\n",
            "[111/600][442]\n",
            "                    Loss_D: 0.61 Loss_G: 47.66 Time: 534.56s\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  70.37030363082886\n",
            "step :  22700 iters_100_time :  120.81300711631775\n",
            "step :  22800 iters_100_time :  120.83402395248413\n",
            "step :  22900 iters_100_time :  120.68810391426086\n",
            "[112/600][442]\n",
            "                    Loss_D: 0.72 Loss_G: 39.36 Time: 534.50s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  19.67331576347351\n",
            "step :  23100 iters_100_time :  120.63791298866272\n",
            "step :  23200 iters_100_time :  121.08631038665771\n",
            "step :  23300 iters_100_time :  120.87117648124695\n",
            "step :  23400 iters_100_time :  120.66423320770264\n",
            "[113/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 58.61 Time: 534.67s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  89.87388825416565\n",
            "step :  23600 iters_100_time :  120.68043684959412\n",
            "step :  23700 iters_100_time :  120.6162965297699\n",
            "step :  23800 iters_100_time :  120.49319839477539\n",
            "[114/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 44.93 Time: 534.05s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  39.05919599533081\n",
            "step :  24000 iters_100_time :  120.5308928489685\n",
            "step :  24100 iters_100_time :  120.49392604827881\n",
            "step :  24200 iters_100_time :  120.2769148349762\n",
            "step :  24300 iters_100_time :  120.34694957733154\n",
            "[115/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 51.12 Time: 533.18s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  108.57345080375671\n",
            "step :  24500 iters_100_time :  120.37108087539673\n",
            "step :  24600 iters_100_time :  120.26134514808655\n",
            "step :  24700 iters_100_time :  120.39030146598816\n",
            "[116/600][442]\n",
            "                    Loss_D: 0.40 Loss_G: 57.08 Time: 532.60s\n",
            "num_batches :  442\n",
            "step :  24800 iters_100_time :  57.981223821640015\n",
            "step :  24900 iters_100_time :  120.35261058807373\n",
            "step :  25000 iters_100_time :  120.27897620201111\n",
            "step :  25000 iters_5000_time :  298.61292243003845\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.050379037857056\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.39779806137085\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  214.15852332115173\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  25100 iters_100_time :  390.10691499710083\n",
            "[117/600][442]\n",
            "                    Loss_D: 1.43 Loss_G: 43.24 Time: 802.10s\n",
            "num_batches :  442\n",
            "step :  25200 iters_100_time :  7.69718861579895\n",
            "step :  25300 iters_100_time :  120.24453139305115\n",
            "step :  25400 iters_100_time :  120.27458596229553\n",
            "step :  25500 iters_100_time :  120.42219948768616\n",
            "step :  25600 iters_100_time :  120.31682586669922\n",
            "[118/600][442]\n",
            "                    Loss_D: 0.60 Loss_G: 61.81 Time: 532.58s\n",
            "num_batches :  442\n",
            "step :  25700 iters_100_time :  77.22892618179321\n",
            "step :  25800 iters_100_time :  120.28730082511902\n",
            "step :  25900 iters_100_time :  120.17625904083252\n",
            "step :  26000 iters_100_time :  120.08680272102356\n",
            "[119/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 50.07 Time: 531.89s\n",
            "num_batches :  442\n",
            "step :  26100 iters_100_time :  26.75953483581543\n",
            "step :  26200 iters_100_time :  120.13025093078613\n",
            "step :  26300 iters_100_time :  119.96221137046814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU78aNl1gQdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fbFB2yZuaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}