{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "2fc1e036-21bb-43e0-b31c-ee1561f24295"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 10 16:05:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    13W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_140.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "136aa946-0b0a-4e22-d8f2-91c291479f75"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_110.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:01<00:00, 65.9MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_110.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS =========  111\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  125.93299674987793\n",
            "step :  200 iters_100_time :  127.74967002868652\n",
            "step :  300 iters_100_time :  127.61299085617065\n",
            "step :  400 iters_100_time :  127.8678035736084\n",
            "[111/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 46.41 Time: 563.05s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  74.45324611663818\n",
            "step :  600 iters_100_time :  127.80528616905212\n",
            "step :  700 iters_100_time :  127.70185708999634\n",
            "step :  800 iters_100_time :  127.63730525970459\n",
            "[112/600][442]\n",
            "                    Loss_D: 1.11 Loss_G: 52.57 Time: 565.24s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  20.843551635742188\n",
            "step :  1000 iters_100_time :  127.65096163749695\n",
            "step :  1100 iters_100_time :  127.67971205711365\n",
            "step :  1200 iters_100_time :  127.42620968818665\n",
            "step :  1300 iters_100_time :  127.55995512008667\n",
            "[113/600][442]\n",
            "                    Loss_D: 0.45 Loss_G: 45.74 Time: 564.79s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  95.03876733779907\n",
            "step :  1500 iters_100_time :  127.49746036529541\n",
            "step :  1600 iters_100_time :  127.72829699516296\n",
            "step :  1700 iters_100_time :  127.66484260559082\n",
            "[114/600][442]\n",
            "                    Loss_D: 0.21 Loss_G: 43.60 Time: 565.26s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  41.21453022956848\n",
            "step :  1900 iters_100_time :  127.70676970481873\n",
            "step :  2000 iters_100_time :  127.6547646522522\n",
            "step :  2100 iters_100_time :  127.5211272239685\n",
            "step :  2200 iters_100_time :  127.59801912307739\n",
            "[115/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 45.25 Time: 564.86s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  115.22119522094727\n",
            "step :  2400 iters_100_time :  127.70256185531616\n",
            "step :  2500 iters_100_time :  127.66209316253662\n",
            "step :  2600 iters_100_time :  127.36785435676575\n",
            "[116/600][442]\n",
            "                    Loss_D: 1.64 Loss_G: 44.12 Time: 564.70s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  61.655383348464966\n",
            "step :  2800 iters_100_time :  127.76322078704834\n",
            "step :  2900 iters_100_time :  127.42253732681274\n",
            "step :  3000 iters_100_time :  127.62629461288452\n",
            "[117/600][442]\n",
            "                    Loss_D: 0.29 Loss_G: 50.24 Time: 564.66s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  7.876146554946899\n",
            "step :  3200 iters_100_time :  127.55452942848206\n",
            "step :  3300 iters_100_time :  127.48877668380737\n",
            "step :  3400 iters_100_time :  127.77998852729797\n",
            "step :  3500 iters_100_time :  127.49238753318787\n",
            "[118/600][442]\n",
            "                    Loss_D: 0.17 Loss_G: 52.28 Time: 564.59s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  81.60996150970459\n",
            "step :  3700 iters_100_time :  127.6284830570221\n",
            "step :  3800 iters_100_time :  127.7903265953064\n",
            "step :  3900 iters_100_time :  127.48902702331543\n",
            "[119/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 61.85 Time: 564.40s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  28.538575410842896\n",
            "step :  4100 iters_100_time :  127.8066656589508\n",
            "step :  4200 iters_100_time :  127.38102149963379\n",
            "step :  4300 iters_100_time :  127.71815705299377\n",
            "step :  4400 iters_100_time :  127.51329183578491\n",
            "[120/600][442]\n",
            "                    Loss_D: 1.31 Loss_G: 52.61 Time: 564.89s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  102.63570809364319\n",
            "step :  4600 iters_100_time :  127.86277103424072\n",
            "step :  4700 iters_100_time :  127.41444420814514\n",
            "step :  4800 iters_100_time :  127.55412244796753\n",
            "[121/600][442]\n",
            "                    Loss_D: 0.44 Loss_G: 47.79 Time: 565.15s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  48.805909156799316\n",
            "step :  5000 iters_100_time :  127.72153377532959\n",
            "step :  5100 iters_100_time :  127.4597578048706\n",
            "step :  5200 iters_100_time :  127.80120277404785\n",
            "step :  5300 iters_100_time :  127.68619847297668\n",
            "[122/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 46.54 Time: 565.07s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  122.97981905937195\n",
            "step :  5500 iters_100_time :  127.6722800731659\n",
            "step :  5600 iters_100_time :  127.59858655929565\n",
            "step :  5700 iters_100_time :  127.68086767196655\n",
            "[123/600][442]\n",
            "                    Loss_D: 0.87 Loss_G: 44.44 Time: 565.04s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  69.25478315353394\n",
            "step :  5900 iters_100_time :  127.6991183757782\n",
            "step :  6000 iters_100_time :  127.73604893684387\n",
            "step :  6100 iters_100_time :  127.7914547920227\n",
            "[124/600][442]\n",
            "                    Loss_D: 1.08 Loss_G: 47.31 Time: 565.28s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  15.668988466262817\n",
            "step :  6300 iters_100_time :  127.65753149986267\n",
            "step :  6400 iters_100_time :  127.72542476654053\n",
            "step :  6500 iters_100_time :  127.82313442230225\n",
            "step :  6600 iters_100_time :  127.86749267578125\n",
            "[125/600][442]\n",
            "                    Loss_D: 0.49 Loss_G: 40.52 Time: 565.63s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  89.8091082572937\n",
            "step :  6800 iters_100_time :  127.5559241771698\n",
            "step :  6900 iters_100_time :  127.59660983085632\n",
            "step :  7000 iters_100_time :  127.94720530509949\n",
            "[126/600][442]\n",
            "                    Loss_D: 0.37 Loss_G: 42.89 Time: 565.46s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  36.14080286026001\n",
            "step :  7200 iters_100_time :  127.57536721229553\n",
            "step :  7300 iters_100_time :  127.75921440124512\n",
            "step :  7400 iters_100_time :  127.63591599464417\n",
            "step :  7500 iters_100_time :  127.76482486724854\n",
            "[127/600][442]\n",
            "                    Loss_D: 1.13 Loss_G: 48.62 Time: 565.19s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  110.2108519077301\n",
            "step :  7700 iters_100_time :  127.48631429672241\n",
            "step :  7800 iters_100_time :  127.7457046508789\n",
            "step :  7900 iters_100_time :  127.53849172592163\n",
            "[128/600][442]\n",
            "                    Loss_D: 2.16 Loss_G: 46.07 Time: 565.04s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  56.626083850860596\n",
            "step :  8100 iters_100_time :  127.7134358882904\n",
            "step :  8200 iters_100_time :  127.62011885643005\n",
            "step :  8300 iters_100_time :  127.53951001167297\n",
            "[129/600][442]\n",
            "                    Loss_D: 1.03 Loss_G: 59.13 Time: 565.06s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.9270036220550537\n",
            "step :  8500 iters_100_time :  127.96109747886658\n",
            "step :  8600 iters_100_time :  127.40328526496887\n",
            "step :  8700 iters_100_time :  127.65877175331116\n",
            "step :  8800 iters_100_time :  127.60054659843445\n",
            "[130/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 56.09 Time: 565.20s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  77.26590251922607\n",
            "step :  9000 iters_100_time :  127.93710899353027\n",
            "step :  9100 iters_100_time :  127.63378667831421\n",
            "step :  9200 iters_100_time :  127.73315787315369\n",
            "[131/600][442]\n",
            "                    Loss_D: 0.32 Loss_G: 61.07 Time: 565.66s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  23.338550567626953\n",
            "step :  9400 iters_100_time :  127.89290595054626\n",
            "step :  9500 iters_100_time :  127.7726833820343\n",
            "step :  9600 iters_100_time :  127.40129542350769\n",
            "step :  9700 iters_100_time :  127.82802271842957\n",
            "[132/600][442]\n",
            "                    Loss_D: 0.53 Loss_G: 64.71 Time: 565.24s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  97.39472699165344\n",
            "step :  9900 iters_100_time :  127.55701398849487\n",
            "step :  10000 iters_100_time :  127.64703512191772\n",
            "step :  10100 iters_100_time :  127.76380944252014\n",
            "[133/600][442]\n",
            "                    Loss_D: 0.28 Loss_G: 49.53 Time: 565.02s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  43.79359722137451\n",
            "step :  10300 iters_100_time :  127.56285166740417\n",
            "step :  10400 iters_100_time :  127.71352958679199\n",
            "step :  10500 iters_100_time :  127.43142867088318\n",
            "step :  10600 iters_100_time :  127.72806692123413\n",
            "[134/600][442]\n",
            "                    Loss_D: 0.57 Loss_G: 42.13 Time: 564.84s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  118.02211618423462\n",
            "step :  10800 iters_100_time :  127.75987672805786\n",
            "step :  10900 iters_100_time :  127.68997526168823\n",
            "step :  11000 iters_100_time :  127.69057106971741\n",
            "[135/600][442]\n",
            "                    Loss_D: 0.99 Loss_G: 64.75 Time: 565.39s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  64.09258913993835\n",
            "step :  11200 iters_100_time :  127.7080385684967\n",
            "step :  11300 iters_100_time :  127.71948957443237\n",
            "step :  11400 iters_100_time :  127.80775237083435\n",
            "[136/600][442]\n",
            "                    Loss_D: 0.22 Loss_G: 64.53 Time: 565.28s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  10.587118864059448\n",
            "step :  11600 iters_100_time :  127.81052660942078\n",
            "step :  11700 iters_100_time :  127.83630919456482\n",
            "step :  11800 iters_100_time :  127.91491222381592\n",
            "step :  11900 iters_100_time :  127.77852702140808\n",
            "[137/600][442]\n",
            "                    Loss_D: 0.42 Loss_G: 58.38 Time: 565.80s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  84.91320896148682\n",
            "step :  12100 iters_100_time :  127.92889618873596\n",
            "step :  12200 iters_100_time :  127.89926028251648\n",
            "step :  12300 iters_100_time :  127.72421669960022\n",
            "[138/600][442]\n",
            "                    Loss_D: 0.28 Loss_G: 41.48 Time: 565.69s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  31.04696488380432\n",
            "step :  12500 iters_100_time :  127.93277955055237\n",
            "step :  12600 iters_100_time :  128.06711435317993\n",
            "step :  12700 iters_100_time :  127.928635597229\n",
            "step :  12800 iters_100_time :  128.07145476341248\n",
            "[139/600][442]\n",
            "                    Loss_D: 0.49 Loss_G: 50.70 Time: 566.44s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  105.35857439041138\n",
            "step :  13000 iters_100_time :  127.93806672096252\n",
            "step :  13100 iters_100_time :  128.00209951400757\n",
            "step :  13200 iters_100_time :  127.63734078407288\n",
            "[140/600][442]\n",
            "                    Loss_D: 0.40 Loss_G: 41.76 Time: 566.10s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  51.87723970413208\n",
            "step :  13400 iters_100_time :  127.80864381790161\n",
            "step :  13500 iters_100_time :  127.74083590507507\n",
            "step :  13600 iters_100_time :  127.80401134490967\n",
            "step :  13700 iters_100_time :  127.8021810054779\n",
            "[141/600][442]\n",
            "                    Loss_D: 0.63 Loss_G: 59.84 Time: 566.19s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  125.9553062915802\n",
            "step :  13900 iters_100_time :  127.79742813110352\n",
            "step :  14000 iters_100_time :  127.98807954788208\n",
            "step :  14100 iters_100_time :  127.89737486839294\n",
            "[142/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 46.31 Time: 566.30s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  72.04313540458679\n",
            "step :  14300 iters_100_time :  127.86890959739685\n",
            "step :  14400 iters_100_time :  127.97895193099976\n",
            "step :  14500 iters_100_time :  128.13432455062866\n",
            "[143/600][442]\n",
            "                    Loss_D: 0.51 Loss_G: 39.55 Time: 566.44s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  18.343562126159668\n",
            "step :  14700 iters_100_time :  127.92702269554138\n",
            "step :  14800 iters_100_time :  127.93463587760925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU78aNl1gQdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fbFB2yZuaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}