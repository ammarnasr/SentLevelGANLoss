{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e31e2806-cfc4-43bb-f4cd-9dd067ae7d53"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 13 09:53:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "37d06cce-837d-4936-d560-3e4ac30c86c5"
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-13 09:55:16--  https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.139, 74.125.195.138, 74.125.195.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4nt57a64vbuaivjhdr1ndtuid2e8ufgk/1597312500000/09657060183789739732/*/1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-13 09:55:46--  https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4nt57a64vbuaivjhdr1ndtuid2e8ufgk/1597312500000/09657060183789739732/*/1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ?e=download\n",
            "Resolving doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘birds.zip’\n",
            "\n",
            "birds.zip               [ <=>                ]   6.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-08-13 09:55:46 (50.5 MB/s) - ‘birds.zip’ saved [6488322]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d3f3689c-bf51-48ff-f1be-81d0366c95dc"
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'bird/': No such file or directory\n",
            "Cloning into 'CUB-Attn-GAN'...\n",
            "remote: Enumerating objects: 292, done.\u001b[K\n",
            "remote: Counting objects: 100% (292/292), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 842 (delta 197), reused 193 (delta 103), pack-reused 550\u001b[K\n",
            "Receiving objects: 100% (842/842), 475.00 MiB | 41.01 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "Checking out files: 100% (117/117), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "0fb2f151-ae82-4497-9e0e-52a65505aae5"
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-13 09:57:37--  https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.113, 74.125.195.100, 74.125.195.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/f04t07237t335qhitbd0jerab71e7orc/1597312650000/17309505201871794426/*/1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-13 09:57:38--  https://doc-04-6c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/f04t07237t335qhitbd0jerab71e7orc/1597312650000/17309505201871794426/*/1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111?e=download\n",
            "Resolving doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-04-6c-docs.googleusercontent.com (doc-04-6c-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257793 (252K) [application/x-rar]\n",
            "Saving to: ‘Pillow.rar’\n",
            "\n",
            "Pillow.rar          100%[===================>] 251.75K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-08-13 09:57:38 (116 MB/s) - ‘Pillow.rar’ saved [257793/257793]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Pillow.rar\n",
            "\n",
            "Creating    Pillow                                                    OK\n",
            "Creating    Pillow/Tests                                              OK\n",
            "Creating    Pillow/Tests/fonts                                        OK\n",
            "Extracting  Pillow/Tests/fonts/FreeMono.ttf                              \b\b\b\b 12%\b\b\b\b 25%\b\b\b\b 38%\b\b\b\b 50%\b\b\b\b 63%\b\b\b\b 76%\b\b\b\b 88%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_330.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9adba0c0-8846-4f41-e9c9-71f71617da7a"
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'SentLevelGANLoss/': No such file or directory\n",
            "Cloning into 'SentLevelGANLoss'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 96 (delta 38), reused 83 (delta 28), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8208591c-fecf-4625-b63b-26c72c36d462"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_260.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:00<00:00, 156MB/s] \n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_260.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS =========  261\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  77.07842898368835\n",
            "step :  200 iters_100_time :  72.66804361343384\n",
            "step :  300 iters_100_time :  73.03289270401001\n",
            "step :  400 iters_100_time :  72.85762810707092\n",
            "[261/600][442]\n",
            "                    Loss_D: 0.40 Loss_G: 44.56 Time: 326.34s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  43.13351082801819\n",
            "step :  600 iters_100_time :  72.96239924430847\n",
            "step :  700 iters_100_time :  73.13921976089478\n",
            "step :  800 iters_100_time :  73.36045289039612\n",
            "[262/600][442]\n",
            "                    Loss_D: 0.71 Loss_G: 55.00 Time: 324.13s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  12.197932004928589\n",
            "step :  1000 iters_100_time :  72.99029445648193\n",
            "step :  1100 iters_100_time :  73.08527898788452\n",
            "step :  1200 iters_100_time :  72.81924343109131\n",
            "step :  1300 iters_100_time :  73.20018362998962\n",
            "[263/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 75.40 Time: 323.39s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  54.66881346702576\n",
            "step :  1500 iters_100_time :  73.31262373924255\n",
            "step :  1600 iters_100_time :  72.9721052646637\n",
            "step :  1700 iters_100_time :  73.12930941581726\n",
            "[264/600][442]\n",
            "                    Loss_D: 0.18 Loss_G: 49.02 Time: 323.89s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  24.21644902229309\n",
            "step :  1900 iters_100_time :  72.70416498184204\n",
            "step :  2000 iters_100_time :  72.93301486968994\n",
            "step :  2100 iters_100_time :  73.17198419570923\n",
            "step :  2200 iters_100_time :  73.13594269752502\n",
            "[265/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 73.39 Time: 323.99s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  66.79401636123657\n",
            "step :  2400 iters_100_time :  73.030592918396\n",
            "step :  2500 iters_100_time :  73.62879037857056\n",
            "step :  2600 iters_100_time :  73.19373178482056\n",
            "[266/600][442]\n",
            "                    Loss_D: 0.34 Loss_G: 43.00 Time: 324.91s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  35.953418254852295\n",
            "step :  2800 iters_100_time :  73.2082884311676\n",
            "step :  2900 iters_100_time :  73.0233862400055\n",
            "step :  3000 iters_100_time :  73.52594137191772\n",
            "[267/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 43.69 Time: 324.53s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  5.008911371231079\n",
            "step :  3200 iters_100_time :  73.05663084983826\n",
            "step :  3300 iters_100_time :  73.074049949646\n",
            "step :  3400 iters_100_time :  73.37852454185486\n",
            "step :  3500 iters_100_time :  73.24746417999268\n",
            "[268/600][442]\n",
            "                    Loss_D: 0.10 Loss_G: 57.12 Time: 324.23s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  47.541959285736084\n",
            "step :  3700 iters_100_time :  73.47060012817383\n",
            "step :  3800 iters_100_time :  73.00190448760986\n",
            "step :  3900 iters_100_time :  73.37715983390808\n",
            "[269/600][442]\n",
            "                    Loss_D: 1.11 Loss_G: 51.74 Time: 324.76s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  17.173075199127197\n",
            "step :  4100 iters_100_time :  72.84560775756836\n",
            "step :  4200 iters_100_time :  73.13687038421631\n",
            "step :  4300 iters_100_time :  72.95163941383362\n",
            "step :  4400 iters_100_time :  73.27304458618164\n",
            "[270/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 53.87 Time: 324.19s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  59.765758991241455\n",
            "step :  4600 iters_100_time :  74.23246192932129\n",
            "step :  4700 iters_100_time :  74.09092450141907\n",
            "step :  4800 iters_100_time :  73.90782809257507\n",
            "[271/600][442]\n",
            "                    Loss_D: 0.23 Loss_G: 73.88 Time: 328.32s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  28.758410453796387\n",
            "step :  5000 iters_100_time :  74.02484846115112\n",
            "step :  5100 iters_100_time :  74.02381014823914\n",
            "step :  5200 iters_100_time :  74.04087352752686\n",
            "step :  5300 iters_100_time :  74.16264200210571\n",
            "[272/600][442]\n",
            "                    Loss_D: 1.95 Loss_G: 51.50 Time: 328.43s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  71.90724515914917\n",
            "step :  5500 iters_100_time :  73.9207091331482\n",
            "step :  5600 iters_100_time :  74.2385995388031\n",
            "step :  5700 iters_100_time :  73.96137428283691\n",
            "[273/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 74.00 Time: 328.34s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  40.54846143722534\n",
            "step :  5900 iters_100_time :  74.51444554328918\n",
            "step :  6000 iters_100_time :  73.98276329040527\n",
            "step :  6100 iters_100_time :  73.85762214660645\n",
            "[274/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 59.02 Time: 328.66s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  9.902067422866821\n",
            "step :  6300 iters_100_time :  74.01035523414612\n",
            "step :  6400 iters_100_time :  74.060054063797\n",
            "step :  6500 iters_100_time :  74.30411052703857\n",
            "step :  6600 iters_100_time :  74.15297985076904\n",
            "[275/600][442]\n",
            "                    Loss_D: 0.27 Loss_G: 42.91 Time: 328.99s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  52.29290270805359\n",
            "step :  6800 iters_100_time :  74.0614423751831\n",
            "step :  6900 iters_100_time :  74.11748480796814\n",
            "step :  7000 iters_100_time :  73.91563248634338\n",
            "[276/600][442]\n",
            "                    Loss_D: 0.23 Loss_G: 49.23 Time: 327.90s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  21.37758755683899\n",
            "step :  7200 iters_100_time :  74.26378345489502\n",
            "step :  7300 iters_100_time :  74.27237367630005\n",
            "step :  7400 iters_100_time :  73.97377252578735\n",
            "step :  7500 iters_100_time :  74.2960433959961\n",
            "[277/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 58.35 Time: 328.67s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  64.79021263122559\n",
            "step :  7700 iters_100_time :  73.96970391273499\n",
            "step :  7800 iters_100_time :  74.78470849990845\n",
            "step :  7900 iters_100_time :  74.12256574630737\n",
            "[278/600][442]\n",
            "                    Loss_D: 0.80 Loss_G: 78.22 Time: 329.61s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  33.373363733291626\n",
            "step :  8100 iters_100_time :  74.84606742858887\n",
            "step :  8200 iters_100_time :  74.56072497367859\n",
            "step :  8300 iters_100_time :  74.26559114456177\n",
            "[279/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 63.45 Time: 330.68s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.1041533946990967\n",
            "step :  8500 iters_100_time :  74.7965795993805\n",
            "step :  8600 iters_100_time :  74.27481698989868\n",
            "step :  8700 iters_100_time :  74.40166211128235\n",
            "step :  8800 iters_100_time :  74.5220959186554\n",
            "[280/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 70.03 Time: 330.27s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  45.19259333610535\n",
            "step :  9000 iters_100_time :  73.64107203483582\n",
            "step :  9100 iters_100_time :  73.62601947784424\n",
            "step :  9200 iters_100_time :  73.85677790641785\n",
            "[281/600][442]\n",
            "                    Loss_D: 2.05 Loss_G: 73.04 Time: 327.27s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  14.057309865951538\n",
            "step :  9400 iters_100_time :  73.5810136795044\n",
            "step :  9500 iters_100_time :  74.01057314872742\n",
            "step :  9600 iters_100_time :  73.74788999557495\n",
            "step :  9700 iters_100_time :  74.18635725975037\n",
            "[282/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 52.97 Time: 327.88s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  57.219258069992065\n",
            "step :  9900 iters_100_time :  73.52011632919312\n",
            "step :  10000 iters_100_time :  73.6228380203247\n",
            "step :  10100 iters_100_time :  73.6014392375946\n",
            "[283/600][442]\n",
            "                    Loss_D: 0.58 Loss_G: 52.38 Time: 326.67s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  25.686699390411377\n",
            "step :  10300 iters_100_time :  73.51103258132935\n",
            "step :  10400 iters_100_time :  73.41811347007751\n",
            "step :  10500 iters_100_time :  73.79647397994995\n",
            "step :  10600 iters_100_time :  73.34302377700806\n",
            "[284/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 62.21 Time: 326.01s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  68.48497080802917\n",
            "step :  10800 iters_100_time :  73.66803860664368\n",
            "step :  10900 iters_100_time :  73.26601314544678\n",
            "step :  11000 iters_100_time :  73.65603375434875\n",
            "[285/600][442]\n",
            "                    Loss_D: 0.44 Loss_G: 49.50 Time: 326.38s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  37.423670053482056\n",
            "step :  11200 iters_100_time :  73.8864233493805\n",
            "step :  11300 iters_100_time :  73.81579375267029\n",
            "step :  11400 iters_100_time :  73.9982659816742\n",
            "[286/600][442]\n",
            "                    Loss_D: 0.42 Loss_G: 58.47 Time: 327.29s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  6.636913537979126\n",
            "step :  11600 iters_100_time :  73.64471340179443\n",
            "step :  11700 iters_100_time :  73.88021636009216\n",
            "step :  11800 iters_100_time :  74.26671314239502\n",
            "step :  11900 iters_100_time :  73.55130910873413\n",
            "[287/600][442]\n",
            "                    Loss_D: 0.53 Loss_G: 69.21 Time: 327.33s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  49.56788158416748\n",
            "step :  12100 iters_100_time :  73.7520318031311\n",
            "step :  12200 iters_100_time :  74.12234306335449\n",
            "step :  12300 iters_100_time :  73.87907481193542\n",
            "[288/600][442]\n",
            "                    Loss_D: 1.10 Loss_G: 69.08 Time: 328.05s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  18.489197492599487\n",
            "step :  12500 iters_100_time :  74.07523655891418\n",
            "step :  12600 iters_100_time :  73.78727388381958\n",
            "step :  12700 iters_100_time :  74.36410284042358\n",
            "step :  12800 iters_100_time :  73.96316885948181\n",
            "[289/600][442]\n",
            "                    Loss_D: 0.83 Loss_G: 74.79 Time: 328.34s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  61.47105097770691\n",
            "step :  13000 iters_100_time :  74.26414608955383\n",
            "step :  13100 iters_100_time :  74.18059539794922\n",
            "step :  13200 iters_100_time :  74.15307760238647\n",
            "[290/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 64.32 Time: 328.63s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  30.802897930145264\n",
            "step :  13400 iters_100_time :  73.70543479919434\n",
            "step :  13500 iters_100_time :  74.18911147117615\n",
            "step :  13600 iters_100_time :  73.73613572120667\n",
            "step :  13700 iters_100_time :  74.10437846183777\n",
            "[291/600][442]\n",
            "                    Loss_D: 0.35 Loss_G: 63.99 Time: 328.62s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  73.19471788406372\n",
            "step :  13900 iters_100_time :  74.04133772850037\n",
            "step :  14000 iters_100_time :  74.23627638816833\n",
            "step :  14100 iters_100_time :  73.77038359642029\n",
            "[292/600][442]\n",
            "                    Loss_D: 0.23 Loss_G: 47.50 Time: 328.16s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  42.185768842697144\n",
            "step :  14300 iters_100_time :  74.77043867111206\n",
            "step :  14400 iters_100_time :  73.61981892585754\n",
            "step :  14500 iters_100_time :  74.27615070343018\n",
            "[293/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 54.33 Time: 328.99s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  11.123811721801758\n",
            "step :  14700 iters_100_time :  74.23520922660828\n",
            "step :  14800 iters_100_time :  73.95881295204163\n",
            "step :  14900 iters_100_time :  74.16497254371643\n",
            "step :  15000 iters_100_time :  74.45086979866028\n",
            "[294/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 38.92 Time: 329.00s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  54.02552676200867\n",
            "step :  15200 iters_100_time :  74.3038923740387\n",
            "step :  15300 iters_100_time :  74.62546348571777\n",
            "step :  15400 iters_100_time :  73.83844423294067\n",
            "[295/600][442]\n",
            "                    Loss_D: 1.11 Loss_G: 42.95 Time: 329.17s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  22.817452430725098\n",
            "step :  15600 iters_100_time :  74.2838294506073\n",
            "step :  15700 iters_100_time :  73.99719095230103\n",
            "step :  15800 iters_100_time :  74.25207543373108\n",
            "step :  15900 iters_100_time :  74.42210340499878\n",
            "[296/600][442]\n",
            "                    Loss_D: 0.98 Loss_G: 49.72 Time: 328.95s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  66.5267608165741\n",
            "step :  16100 iters_100_time :  73.92691206932068\n",
            "step :  16200 iters_100_time :  74.42342615127563\n",
            "step :  16300 iters_100_time :  74.54444670677185\n",
            "[297/600][442]\n",
            "                    Loss_D: 0.80 Loss_G: 59.00 Time: 330.07s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  34.86996507644653\n",
            "step :  16500 iters_100_time :  74.64150071144104\n",
            "step :  16600 iters_100_time :  74.17580676078796\n",
            "step :  16700 iters_100_time :  74.25402736663818\n",
            "[298/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 49.97 Time: 329.70s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  3.334740400314331\n",
            "step :  16900 iters_100_time :  74.44042658805847\n",
            "step :  17000 iters_100_time :  74.57180047035217\n",
            "step :  17100 iters_100_time :  73.88593482971191\n",
            "step :  17200 iters_100_time :  74.6313545703888\n",
            "[299/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 55.78 Time: 329.37s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  46.68532204627991\n",
            "step :  17400 iters_100_time :  73.88171219825745\n",
            "step :  17500 iters_100_time :  74.37308144569397\n",
            "step :  17600 iters_100_time :  73.40967082977295\n",
            "[300/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 45.49 Time: 328.35s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  15.617000818252563\n",
            "step :  17800 iters_100_time :  73.96345567703247\n",
            "step :  17900 iters_100_time :  73.80564188957214\n",
            "step :  18000 iters_100_time :  74.32667946815491\n",
            "step :  18100 iters_100_time :  73.99835348129272\n",
            "[301/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 74.62 Time: 328.70s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  58.79815149307251\n",
            "step :  18300 iters_100_time :  74.49341797828674\n",
            "step :  18400 iters_100_time :  73.68186902999878\n",
            "step :  18500 iters_100_time :  74.68463158607483\n",
            "[302/600][442]\n",
            "                    Loss_D: 0.16 Loss_G: 51.16 Time: 329.46s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  27.460086822509766\n",
            "step :  18700 iters_100_time :  74.14373207092285\n",
            "step :  18800 iters_100_time :  74.22729802131653\n",
            "step :  18900 iters_100_time :  74.40066957473755\n",
            "step :  19000 iters_100_time :  74.63717675209045\n",
            "[303/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 49.36 Time: 329.56s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  70.57265973091125\n",
            "step :  19200 iters_100_time :  74.09997177124023\n",
            "step :  19300 iters_100_time :  74.6057665348053\n",
            "step :  19400 iters_100_time :  74.14248871803284\n",
            "[304/600][442]\n",
            "                    Loss_D: 1.48 Loss_G: 58.92 Time: 329.44s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  39.47381520271301\n",
            "step :  19600 iters_100_time :  74.08468675613403\n",
            "step :  19700 iters_100_time :  74.23307037353516\n",
            "step :  19800 iters_100_time :  74.20438599586487\n",
            "[305/600][442]\n",
            "                    Loss_D: 0.43 Loss_G: 58.64 Time: 329.79s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  8.04205870628357\n",
            "step :  20000 iters_100_time :  74.41488575935364\n",
            "step :  20100 iters_100_time :  74.19651436805725\n",
            "step :  20200 iters_100_time :  74.21234273910522\n",
            "step :  20300 iters_100_time :  74.117182970047\n",
            "[306/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 45.79 Time: 329.17s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  51.41468572616577\n",
            "step :  20500 iters_100_time :  74.28193473815918\n",
            "step :  20600 iters_100_time :  74.08679580688477\n",
            "step :  20700 iters_100_time :  74.01103687286377\n",
            "[307/600][442]\n",
            "                    Loss_D: 0.83 Loss_G: 43.69 Time: 329.14s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  19.800485849380493\n",
            "step :  20900 iters_100_time :  74.58376908302307\n",
            "step :  21000 iters_100_time :  74.49917888641357\n",
            "step :  21100 iters_100_time :  74.14344716072083\n",
            "step :  21200 iters_100_time :  74.63082122802734\n",
            "[308/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 61.80 Time: 329.52s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  63.26368975639343\n",
            "step :  21400 iters_100_time :  74.61283993721008\n",
            "step :  21500 iters_100_time :  74.39900183677673\n",
            "step :  21600 iters_100_time :  74.03187417984009\n",
            "[309/600][442]\n",
            "                    Loss_D: 0.21 Loss_G: 57.01 Time: 329.82s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  32.0742871761322\n",
            "step :  21800 iters_100_time :  74.12967681884766\n",
            "step :  21900 iters_100_time :  74.28243136405945\n",
            "step :  22000 iters_100_time :  74.45594549179077\n",
            "step :  22100 iters_100_time :  74.24023509025574\n",
            "[310/600][442]\n",
            "                    Loss_D: 0.33 Loss_G: 56.71 Time: 329.53s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  74.67934679985046\n",
            "step :  22300 iters_100_time :  74.23622608184814\n",
            "step :  22400 iters_100_time :  73.47437834739685\n",
            "step :  22500 iters_100_time :  74.66342234611511\n",
            "[311/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 55.01 Time: 328.71s\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  43.414690017700195\n",
            "step :  22700 iters_100_time :  74.27113580703735\n",
            "step :  22800 iters_100_time :  74.33144736289978\n",
            "step :  22900 iters_100_time :  74.12196946144104\n",
            "[312/600][442]\n",
            "                    Loss_D: 0.10 Loss_G: 58.70 Time: 328.72s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  12.812464475631714\n",
            "step :  23100 iters_100_time :  74.17264461517334\n",
            "step :  23200 iters_100_time :  74.41655278205872\n",
            "step :  23300 iters_100_time :  74.67814469337463\n",
            "step :  23400 iters_100_time :  74.06245756149292\n",
            "[313/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 52.23 Time: 329.99s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  55.77656149864197\n",
            "step :  23600 iters_100_time :  74.27975058555603\n",
            "step :  23700 iters_100_time :  74.58548092842102\n",
            "step :  23800 iters_100_time :  74.08336520195007\n",
            "[314/600][442]\n",
            "                    Loss_D: 0.21 Loss_G: 51.42 Time: 329.98s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  24.382444620132446\n",
            "step :  24000 iters_100_time :  74.49435758590698\n",
            "step :  24100 iters_100_time :  73.83782243728638\n",
            "step :  24200 iters_100_time :  74.44572257995605\n",
            "step :  24300 iters_100_time :  74.5881896018982\n",
            "[315/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 76.34 Time: 329.70s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  67.3749029636383\n",
            "step :  24500 iters_100_time :  74.45830059051514\n",
            "step :  24600 iters_100_time :  74.27795076370239\n",
            "step :  24700 iters_100_time :  74.51162195205688\n",
            "[316/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 65.47 Time: 329.43s\n",
            "num_batches :  442\n",
            "step :  24800 iters_100_time :  36.62281823158264\n",
            "step :  24900 iters_100_time :  74.46999168395996\n",
            "step :  25000 iters_100_time :  74.49699211120605\n",
            "step :  25000 iters_5000_time :  185.5899202823639\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  6.88112998008728\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.239781141281128\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  198.0135247707367\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  25100 iters_100_time :  324.70335364341736\n",
            "[317/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 52.86 Time: 580.60s\n",
            "num_batches :  442\n",
            "step :  25200 iters_100_time :  5.067228317260742\n",
            "step :  25300 iters_100_time :  74.41299676895142\n",
            "step :  25400 iters_100_time :  74.17111587524414\n",
            "step :  25500 iters_100_time :  74.09857487678528\n",
            "step :  25600 iters_100_time :  74.30741834640503\n",
            "[318/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 59.44 Time: 329.63s\n",
            "num_batches :  442\n",
            "step :  25700 iters_100_time :  48.14922046661377\n",
            "step :  25800 iters_100_time :  74.77309679985046\n",
            "step :  25900 iters_100_time :  73.91153860092163\n",
            "step :  26000 iters_100_time :  74.31825280189514\n",
            "[319/600][442]\n",
            "                    Loss_D: 0.32 Loss_G: 61.60 Time: 329.78s\n",
            "num_batches :  442\n",
            "step :  26100 iters_100_time :  16.84272861480713\n",
            "step :  26200 iters_100_time :  74.3215799331665\n",
            "step :  26300 iters_100_time :  74.94329571723938\n",
            "step :  26400 iters_100_time :  74.00650024414062\n",
            "step :  26500 iters_100_time :  74.28799772262573\n",
            "[320/600][442]\n",
            "                    Loss_D: 0.63 Loss_G: 52.13 Time: 329.71s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  26600 iters_100_time :  60.53425407409668\n",
            "step :  26700 iters_100_time :  74.19616341590881\n",
            "step :  26800 iters_100_time :  74.45578598976135\n",
            "step :  26900 iters_100_time :  74.3742847442627\n",
            "[321/600][442]\n",
            "                    Loss_D: 0.51 Loss_G: 68.00 Time: 330.42s\n",
            "num_batches :  442\n",
            "step :  27000 iters_100_time :  28.707398414611816\n",
            "step :  27100 iters_100_time :  74.50327706336975\n",
            "step :  27200 iters_100_time :  74.13070034980774\n",
            "step :  27300 iters_100_time :  74.71284341812134\n",
            "step :  27400 iters_100_time :  74.34097647666931\n",
            "[322/600][442]\n",
            "                    Loss_D: 1.55 Loss_G: 63.83 Time: 329.85s\n",
            "num_batches :  442\n",
            "step :  27500 iters_100_time :  72.44281768798828\n",
            "step :  27600 iters_100_time :  74.2832977771759\n",
            "step :  27700 iters_100_time :  74.10788607597351\n",
            "step :  27800 iters_100_time :  74.60195398330688\n",
            "[323/600][442]\n",
            "                    Loss_D: 0.78 Loss_G: 64.09 Time: 329.77s\n",
            "num_batches :  442\n",
            "step :  27900 iters_100_time :  40.78608059883118\n",
            "step :  28000 iters_100_time :  74.10002112388611\n",
            "step :  28100 iters_100_time :  74.43605256080627\n",
            "step :  28200 iters_100_time :  73.88007164001465\n",
            "[324/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 45.93 Time: 328.94s\n",
            "num_batches :  442\n",
            "step :  28300 iters_100_time :  9.699283599853516\n",
            "step :  28400 iters_100_time :  74.22650051116943\n",
            "step :  28500 iters_100_time :  74.36971545219421\n",
            "step :  28600 iters_100_time :  74.27716398239136\n",
            "step :  28700 iters_100_time :  74.00302267074585\n",
            "[325/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 46.02 Time: 329.45s\n",
            "num_batches :  442\n",
            "step :  28800 iters_100_time :  52.8692090511322\n",
            "step :  28900 iters_100_time :  73.98739957809448\n",
            "step :  29000 iters_100_time :  74.48762893676758\n",
            "step :  29100 iters_100_time :  74.40468740463257\n",
            "[326/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 58.10 Time: 329.33s\n",
            "num_batches :  442\n",
            "step :  29200 iters_100_time :  21.390292167663574\n",
            "step :  29300 iters_100_time :  74.41471695899963\n",
            "step :  29400 iters_100_time :  74.15147590637207\n",
            "step :  29500 iters_100_time :  74.42490744590759\n",
            "step :  29600 iters_100_time :  73.9664556980133\n",
            "[327/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 63.25 Time: 329.36s\n",
            "num_batches :  442\n",
            "step :  29700 iters_100_time :  64.59004998207092\n",
            "step :  29800 iters_100_time :  74.65813112258911\n",
            "step :  29900 iters_100_time :  73.89690041542053\n",
            "step :  30000 iters_100_time :  74.54484748840332\n",
            "[328/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 47.03 Time: 329.83s\n",
            "num_batches :  442\n",
            "step :  30100 iters_100_time :  33.2561571598053\n",
            "step :  30200 iters_100_time :  74.38931083679199\n",
            "step :  30300 iters_100_time :  74.69624614715576\n",
            "step :  30400 iters_100_time :  74.14573693275452\n",
            "[329/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 42.43 Time: 329.99s\n",
            "num_batches :  442\n",
            "step :  30500 iters_100_time :  2.1455142498016357\n",
            "step :  30600 iters_100_time :  74.36943173408508\n",
            "step :  30700 iters_100_time :  74.50602316856384\n",
            "step :  30800 iters_100_time :  74.8275876045227\n",
            "step :  30900 iters_100_time :  73.91009902954102\n",
            "[330/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 62.30 Time: 329.79s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  31000 iters_100_time :  45.76399874687195\n",
            "step :  31100 iters_100_time :  74.56088137626648\n",
            "step :  31200 iters_100_time :  73.93585062026978\n",
            "step :  31300 iters_100_time :  74.56039452552795\n",
            "[331/600][442]\n",
            "                    Loss_D: 1.30 Loss_G: 76.31 Time: 330.30s\n",
            "num_batches :  442\n",
            "step :  31400 iters_100_time :  13.943376541137695\n",
            "step :  31500 iters_100_time :  74.6090157032013\n",
            "step :  31600 iters_100_time :  74.74924635887146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBwYuwliPq-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}