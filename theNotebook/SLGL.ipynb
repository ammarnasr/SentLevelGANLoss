{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKs7S4PZR2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzcYrCvZuQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "!rm -r sample_data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkB9zZRNbw9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clone repo AttnGAN\n",
        "os.chdir('/content/')\n",
        "!rm -r AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj7VSMY-c6uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download captionts filenames and classes info\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDk_GzlMduaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3rHv8ycCAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dwonload files text encoder and image encoder\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "\n",
        "# #Move Models text and image encoder to their /content/\n",
        "!mv  /content/CUB-Attn-GAN/theModel/text_encoder599.pth  /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "\n",
        "!rm -r CUB-Attn-GAN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDFg63INfn0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download Pillow Font\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTLsxwtly4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netG_epoch_400.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/ModifiedcubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESi47FqIgLlw",
        "colab_type": "text"
      },
      "source": [
        "# =============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTJmFZTgLPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Move code files to their Locations\n",
        "os.chdir('/content')\n",
        "!rm -r SentLevelGANLoss/\n",
        "!git clone https://github.com/ammarnasr/SentLevelGANLoss.git\n",
        "\n",
        "!mv /content/SentLevelGANLoss/theCode/config.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/utils.py                      /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/datasets.py                  /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/GlobalAttention.py          /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/model.py                   /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/losses.py                 /content/AttnGAN/code/miscc/\n",
        "!mv /content/SentLevelGANLoss/theCode/trainer.py               /content/AttnGAN/code/\n",
        "!mv /content/SentLevelGANLoss/theCode/bird_attn2.yml          /content/AttnGAN/code/cfg/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btCJ5ibpf7rZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39f6115a-df98-4071-b02f-095fdb97860d"
      },
      "source": [
        "#run Code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_330.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:00<00:00, 268MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_330.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS =========  331\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/content/AttnGAN/code/GlobalAttention.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  sent_att                = nn.Softmax()(sentence_vs)  # batch x idf x ih x iw\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:428: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  79.41800665855408\n",
            "step :  200 iters_100_time :  75.59396457672119\n",
            "step :  300 iters_100_time :  75.32865881919861\n",
            "step :  400 iters_100_time :  75.06197810173035\n",
            "[331/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 70.58 Time: 337.29s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  44.08815908432007\n",
            "step :  600 iters_100_time :  75.02932381629944\n",
            "step :  700 iters_100_time :  75.12312602996826\n",
            "step :  800 iters_100_time :  75.3138074874878\n",
            "[332/600][442]\n",
            "                    Loss_D: 2.82 Loss_G: 32.32 Time: 332.74s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  12.83574366569519\n",
            "step :  1000 iters_100_time :  75.2783784866333\n",
            "step :  1100 iters_100_time :  74.89113235473633\n",
            "step :  1200 iters_100_time :  75.58616280555725\n",
            "step :  1300 iters_100_time :  75.0299232006073\n",
            "[333/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 68.63 Time: 333.19s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  56.37201547622681\n",
            "step :  1500 iters_100_time :  75.4799120426178\n",
            "step :  1600 iters_100_time :  75.26733875274658\n",
            "step :  1700 iters_100_time :  75.29079461097717\n",
            "[334/600][442]\n",
            "                    Loss_D: 0.16 Loss_G: 62.69 Time: 333.88s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  25.123603105545044\n",
            "step :  1900 iters_100_time :  74.83000063896179\n",
            "step :  2000 iters_100_time :  74.88830661773682\n",
            "step :  2100 iters_100_time :  75.64839243888855\n",
            "step :  2200 iters_100_time :  74.99452328681946\n",
            "[335/600][442]\n",
            "                    Loss_D: 0.60 Loss_G: 64.48 Time: 333.46s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  67.96412062644958\n",
            "step :  2400 iters_100_time :  75.27583265304565\n",
            "step :  2500 iters_100_time :  75.2406919002533\n",
            "step :  2600 iters_100_time :  74.98731064796448\n",
            "[336/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 59.90 Time: 333.03s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  36.76624536514282\n",
            "step :  2800 iters_100_time :  75.64268231391907\n",
            "step :  2900 iters_100_time :  75.41978192329407\n",
            "step :  3000 iters_100_time :  75.52487444877625\n",
            "[337/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 54.71 Time: 335.05s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  5.495372295379639\n",
            "step :  3200 iters_100_time :  75.69425392150879\n",
            "step :  3300 iters_100_time :  75.08302164077759\n",
            "step :  3400 iters_100_time :  76.00532293319702\n",
            "step :  3500 iters_100_time :  75.80638813972473\n",
            "[338/600][442]\n",
            "                    Loss_D: 0.89 Loss_G: 65.43 Time: 335.65s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  48.973628282547\n",
            "step :  3700 iters_100_time :  75.64342880249023\n",
            "step :  3800 iters_100_time :  75.12291359901428\n",
            "step :  3900 iters_100_time :  75.33848547935486\n",
            "[339/600][442]\n",
            "                    Loss_D: 0.16 Loss_G: 47.01 Time: 334.41s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  17.706443548202515\n",
            "step :  4100 iters_100_time :  75.13320255279541\n",
            "step :  4200 iters_100_time :  75.16392612457275\n",
            "step :  4300 iters_100_time :  75.60266280174255\n",
            "step :  4400 iters_100_time :  75.48741006851196\n",
            "[340/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 65.69 Time: 334.34s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  60.376811504364014\n",
            "step :  4600 iters_100_time :  73.91383719444275\n",
            "step :  4700 iters_100_time :  73.89806628227234\n",
            "step :  4800 iters_100_time :  74.41361260414124\n",
            "[341/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 38.09 Time: 328.93s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  28.762715816497803\n",
            "step :  5000 iters_100_time :  74.51980471611023\n",
            "step :  5100 iters_100_time :  74.70545959472656\n",
            "step :  5200 iters_100_time :  73.9704909324646\n",
            "step :  5300 iters_100_time :  74.4786319732666\n",
            "[342/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 54.24 Time: 329.52s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  72.16944432258606\n",
            "step :  5500 iters_100_time :  74.11588287353516\n",
            "step :  5600 iters_100_time :  74.49160122871399\n",
            "step :  5700 iters_100_time :  74.33554530143738\n",
            "[343/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 55.04 Time: 330.01s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  41.21448040008545\n",
            "step :  5900 iters_100_time :  74.39354038238525\n",
            "step :  6000 iters_100_time :  74.69726157188416\n",
            "step :  6100 iters_100_time :  74.63235998153687\n",
            "[344/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 63.67 Time: 330.51s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  9.601286172866821\n",
            "step :  6300 iters_100_time :  74.78517413139343\n",
            "step :  6400 iters_100_time :  74.47118878364563\n",
            "step :  6500 iters_100_time :  74.29881358146667\n",
            "step :  6600 iters_100_time :  74.52217745780945\n",
            "[345/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 60.79 Time: 330.29s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  52.993576765060425\n",
            "step :  6800 iters_100_time :  74.63427877426147\n",
            "step :  6900 iters_100_time :  74.45527195930481\n",
            "step :  7000 iters_100_time :  74.70106768608093\n",
            "[346/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 43.44 Time: 330.83s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  21.47612762451172\n",
            "step :  7200 iters_100_time :  74.58868312835693\n",
            "step :  7300 iters_100_time :  74.90250968933105\n",
            "step :  7400 iters_100_time :  74.10272192955017\n",
            "step :  7500 iters_100_time :  74.64909386634827\n",
            "[347/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 46.86 Time: 330.48s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  65.10337805747986\n",
            "step :  7700 iters_100_time :  74.59894061088562\n",
            "step :  7800 iters_100_time :  74.51385951042175\n",
            "step :  7900 iters_100_time :  74.33350086212158\n",
            "[348/600][442]\n",
            "                    Loss_D: 0.71 Loss_G: 52.69 Time: 331.10s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  33.89057111740112\n",
            "step :  8100 iters_100_time :  74.35511636734009\n",
            "step :  8200 iters_100_time :  74.8839020729065\n",
            "step :  8300 iters_100_time :  74.68747019767761\n",
            "[349/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 57.81 Time: 331.04s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.1124489307403564\n",
            "step :  8500 iters_100_time :  74.97110342979431\n",
            "step :  8600 iters_100_time :  74.40117239952087\n",
            "step :  8700 iters_100_time :  74.54629254341125\n",
            "step :  8800 iters_100_time :  75.04507207870483\n",
            "[350/600][442]\n",
            "                    Loss_D: 0.13 Loss_G: 79.34 Time: 331.08s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  45.320703983306885\n",
            "step :  9000 iters_100_time :  74.694020986557\n",
            "step :  9100 iters_100_time :  75.0780668258667\n",
            "step :  9200 iters_100_time :  74.3328149318695\n",
            "[351/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 48.06 Time: 331.46s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  14.102123260498047\n",
            "step :  9400 iters_100_time :  74.6933856010437\n",
            "step :  9500 iters_100_time :  74.54373002052307\n",
            "step :  9600 iters_100_time :  74.9969232082367\n",
            "step :  9700 iters_100_time :  74.50827145576477\n",
            "[352/600][442]\n",
            "                    Loss_D: 0.38 Loss_G: 67.21 Time: 331.31s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  57.59867572784424\n",
            "step :  9900 iters_100_time :  74.55105590820312\n",
            "step :  10000 iters_100_time :  74.58305263519287\n",
            "step :  10100 iters_100_time :  74.80086135864258\n",
            "[353/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 46.13 Time: 331.09s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  25.794066429138184\n",
            "step :  10300 iters_100_time :  75.01154851913452\n",
            "step :  10400 iters_100_time :  74.54228925704956\n",
            "step :  10500 iters_100_time :  74.50094366073608\n",
            "step :  10600 iters_100_time :  74.68891453742981\n",
            "[354/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 58.00 Time: 330.87s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  69.27007174491882\n",
            "step :  10800 iters_100_time :  74.81726741790771\n",
            "step :  10900 iters_100_time :  74.17063188552856\n",
            "step :  11000 iters_100_time :  74.80128717422485\n",
            "[355/600][442]\n",
            "                    Loss_D: 0.43 Loss_G: 46.52 Time: 331.03s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  37.683422565460205\n",
            "step :  11200 iters_100_time :  74.86466360092163\n",
            "step :  11300 iters_100_time :  74.94049882888794\n",
            "step :  11400 iters_100_time :  74.53971815109253\n",
            "[356/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 50.59 Time: 331.21s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  6.68626856803894\n",
            "step :  11600 iters_100_time :  74.61160802841187\n",
            "step :  11700 iters_100_time :  75.11568093299866\n",
            "step :  11800 iters_100_time :  74.82453536987305\n",
            "step :  11900 iters_100_time :  74.43472385406494\n",
            "[357/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 61.20 Time: 331.59s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  50.163933753967285\n",
            "step :  12100 iters_100_time :  74.53961396217346\n",
            "step :  12200 iters_100_time :  74.84233379364014\n",
            "step :  12300 iters_100_time :  74.93464231491089\n",
            "[358/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 58.89 Time: 331.31s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  18.420990705490112\n",
            "step :  12500 iters_100_time :  75.24395298957825\n",
            "step :  12600 iters_100_time :  74.69651317596436\n",
            "step :  12700 iters_100_time :  74.88392329216003\n",
            "step :  12800 iters_100_time :  74.93509411811829\n",
            "[359/600][442]\n",
            "                    Loss_D: 3.41 Loss_G: 50.58 Time: 331.91s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  61.896430015563965\n",
            "step :  13000 iters_100_time :  75.09721446037292\n",
            "step :  13100 iters_100_time :  74.34558415412903\n",
            "step :  13200 iters_100_time :  74.86488175392151\n",
            "[360/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 68.68 Time: 331.26s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  31.03313946723938\n",
            "step :  13400 iters_100_time :  74.70603775978088\n",
            "step :  13500 iters_100_time :  74.78887915611267\n",
            "step :  13600 iters_100_time :  75.37017679214478\n",
            "step :  13700 iters_100_time :  74.86678504943848\n",
            "[361/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 59.28 Time: 332.77s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  74.8242735862732\n",
            "step :  13900 iters_100_time :  74.81977534294128\n",
            "step :  14000 iters_100_time :  74.88985395431519\n",
            "step :  14100 iters_100_time :  74.84439945220947\n",
            "[362/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 55.90 Time: 332.93s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  42.68051481246948\n",
            "step :  14300 iters_100_time :  74.99018216133118\n",
            "step :  14400 iters_100_time :  74.86855149269104\n",
            "step :  14500 iters_100_time :  75.15621542930603\n",
            "[363/600][442]\n",
            "                    Loss_D: 0.81 Loss_G: 68.61 Time: 332.32s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  11.257555484771729\n",
            "step :  14700 iters_100_time :  74.79327297210693\n",
            "step :  14800 iters_100_time :  75.13936853408813\n",
            "step :  14900 iters_100_time :  74.65163922309875\n",
            "step :  15000 iters_100_time :  75.07270765304565\n",
            "[364/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 74.23 Time: 332.45s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  54.43431377410889\n",
            "step :  15200 iters_100_time :  75.51381134986877\n",
            "step :  15300 iters_100_time :  74.89612174034119\n",
            "step :  15400 iters_100_time :  74.62759923934937\n",
            "[365/600][442]\n",
            "                    Loss_D: 2.55 Loss_G: 60.33 Time: 332.34s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  23.602494955062866\n",
            "step :  15600 iters_100_time :  74.52017831802368\n",
            "step :  15700 iters_100_time :  74.91251063346863\n",
            "step :  15800 iters_100_time :  75.38260173797607\n",
            "step :  15900 iters_100_time :  75.05755758285522\n",
            "[366/600][442]\n",
            "                    Loss_D: 0.33 Loss_G: 59.15 Time: 332.91s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  66.67884516716003\n",
            "step :  16100 iters_100_time :  74.7317943572998\n",
            "step :  16200 iters_100_time :  75.12852716445923\n",
            "step :  16300 iters_100_time :  75.28926968574524\n",
            "[367/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 46.11 Time: 332.56s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  35.02519774436951\n",
            "step :  16500 iters_100_time :  75.3860399723053\n",
            "step :  16600 iters_100_time :  74.53230333328247\n",
            "step :  16700 iters_100_time :  74.9814965724945\n",
            "[368/600][442]\n",
            "                    Loss_D: 4.84 Loss_G: 71.32 Time: 332.31s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  3.4862053394317627\n",
            "step :  16900 iters_100_time :  74.98551416397095\n",
            "step :  17000 iters_100_time :  75.30231404304504\n",
            "step :  17100 iters_100_time :  74.75090980529785\n",
            "step :  17200 iters_100_time :  74.9271149635315\n",
            "[369/600][442]\n",
            "                    Loss_D: 0.16 Loss_G: 58.69 Time: 332.17s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  46.910378217697144\n",
            "step :  17400 iters_100_time :  75.13413047790527\n",
            "step :  17500 iters_100_time :  75.4144937992096\n",
            "step :  17600 iters_100_time :  74.4870502948761\n",
            "[370/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 46.98 Time: 332.73s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  15.63363766670227\n",
            "step :  17800 iters_100_time :  75.3636531829834\n",
            "step :  17900 iters_100_time :  74.69557023048401\n",
            "step :  18000 iters_100_time :  75.03516435623169\n",
            "step :  18100 iters_100_time :  75.00149369239807\n",
            "[371/600][442]\n",
            "                    Loss_D: 0.63 Loss_G: 64.74 Time: 332.71s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  58.94166374206543\n",
            "step :  18300 iters_100_time :  75.32813477516174\n",
            "step :  18400 iters_100_time :  75.37754726409912\n",
            "step :  18500 iters_100_time :  74.89832735061646\n",
            "[372/600][442]\n",
            "                    Loss_D: 0.41 Loss_G: 61.81 Time: 333.19s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  27.23754620552063\n",
            "step :  18700 iters_100_time :  75.42350625991821\n",
            "step :  18800 iters_100_time :  75.21690702438354\n",
            "step :  18900 iters_100_time :  74.77059316635132\n",
            "step :  19000 iters_100_time :  75.3645327091217\n",
            "[373/600][442]\n",
            "                    Loss_D: 0.16 Loss_G: 60.24 Time: 332.67s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  71.31684899330139\n",
            "step :  19200 iters_100_time :  75.03871059417725\n",
            "step :  19300 iters_100_time :  75.43564963340759\n",
            "step :  19400 iters_100_time :  74.95027208328247\n",
            "[374/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 55.79 Time: 333.08s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  39.69309067726135\n",
            "step :  19600 iters_100_time :  75.12229943275452\n",
            "step :  19700 iters_100_time :  75.38789129257202\n",
            "step :  19800 iters_100_time :  75.03978323936462\n",
            "[375/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 56.67 Time: 333.05s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  8.010311603546143\n",
            "step :  20000 iters_100_time :  75.6418399810791\n",
            "step :  20100 iters_100_time :  75.20351266860962\n",
            "step :  20200 iters_100_time :  75.15217709541321\n",
            "step :  20300 iters_100_time :  75.13072371482849\n",
            "[376/600][442]\n",
            "                    Loss_D: 0.98 Loss_G: 54.59 Time: 333.57s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  51.732075214385986\n",
            "step :  20500 iters_100_time :  75.3735580444336\n",
            "step :  20600 iters_100_time :  74.85555028915405\n",
            "step :  20700 iters_100_time :  75.3459985256195\n",
            "[377/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 75.57 Time: 333.30s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  20.32047462463379\n",
            "step :  20900 iters_100_time :  75.07224678993225\n",
            "step :  21000 iters_100_time :  75.68305563926697\n",
            "step :  21100 iters_100_time :  74.84353590011597\n",
            "step :  21200 iters_100_time :  75.52718925476074\n",
            "[378/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 61.91 Time: 334.04s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  64.11361122131348\n",
            "step :  21400 iters_100_time :  75.10422587394714\n",
            "step :  21500 iters_100_time :  75.82446384429932\n",
            "step :  21600 iters_100_time :  75.01400065422058\n",
            "[379/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 64.68 Time: 334.25s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  32.507596492767334\n",
            "step :  21800 iters_100_time :  75.0319983959198\n",
            "step :  21900 iters_100_time :  75.3751974105835\n",
            "step :  22000 iters_100_time :  75.5863687992096\n",
            "step :  22100 iters_100_time :  74.83835554122925\n",
            "[380/600][442]\n",
            "                    Loss_D: 0.55 Loss_G: 72.75 Time: 333.68s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  75.96626114845276\n",
            "step :  22300 iters_100_time :  75.00433421134949\n",
            "step :  22400 iters_100_time :  75.4591417312622\n",
            "step :  22500 iters_100_time :  75.3815655708313\n",
            "[381/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 49.34 Time: 333.96s\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  44.283658027648926\n",
            "step :  22700 iters_100_time :  75.56324005126953\n",
            "step :  22800 iters_100_time :  75.12722682952881\n",
            "step :  22900 iters_100_time :  75.33775568008423\n",
            "[382/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 57.19 Time: 333.59s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  12.682147979736328\n",
            "step :  23100 iters_100_time :  75.07113099098206\n",
            "step :  23200 iters_100_time :  75.52495169639587\n",
            "step :  23300 iters_100_time :  75.01658821105957\n",
            "step :  23400 iters_100_time :  75.2434093952179\n",
            "[383/600][442]\n",
            "                    Loss_D: 0.09 Loss_G: 55.58 Time: 333.34s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  56.44708013534546\n",
            "step :  23600 iters_100_time :  75.08513140678406\n",
            "step :  23700 iters_100_time :  75.70694303512573\n",
            "step :  23800 iters_100_time :  74.82492065429688\n",
            "[384/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 62.89 Time: 333.76s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  24.394580125808716\n",
            "step :  24000 iters_100_time :  75.23676538467407\n",
            "step :  24100 iters_100_time :  75.74016642570496\n",
            "step :  24200 iters_100_time :  75.18356251716614\n",
            "step :  24300 iters_100_time :  74.98703861236572\n",
            "[385/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 58.20 Time: 333.52s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  68.31788873672485\n",
            "step :  24500 iters_100_time :  75.49485659599304\n",
            "step :  24600 iters_100_time :  75.58947825431824\n",
            "step :  24700 iters_100_time :  75.31599640846252\n",
            "[386/600][442]\n",
            "                    Loss_D: 1.32 Loss_G: 62.58 Time: 334.20s\n",
            "num_batches :  442\n",
            "step :  24800 iters_100_time :  36.94387149810791\n",
            "step :  24900 iters_100_time :  75.48883390426636\n",
            "step :  25000 iters_100_time :  75.3248643875122\n",
            "step :  25000 iters_5000_time :  187.75768947601318\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.056535482406616\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  27.250020027160645\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  204.7529125213623\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  25100 iters_100_time :  336.0699691772461\n",
            "[387/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 58.53 Time: 594.85s\n",
            "num_batches :  442\n",
            "step :  25200 iters_100_time :  5.151780128479004\n",
            "step :  25300 iters_100_time :  75.23415803909302\n",
            "step :  25400 iters_100_time :  75.57636094093323\n",
            "step :  25500 iters_100_time :  75.200190782547\n",
            "step :  25600 iters_100_time :  75.54348921775818\n",
            "[388/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 55.66 Time: 334.27s\n",
            "num_batches :  442\n",
            "step :  25700 iters_100_time :  48.73901557922363\n",
            "step :  25800 iters_100_time :  75.75995135307312\n",
            "step :  25900 iters_100_time :  75.25633716583252\n",
            "step :  26000 iters_100_time :  75.51398038864136\n",
            "[389/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 57.06 Time: 334.32s\n",
            "num_batches :  442\n",
            "step :  26100 iters_100_time :  17.58933997154236\n",
            "step :  26200 iters_100_time :  75.49065828323364\n",
            "step :  26300 iters_100_time :  75.53324866294861\n",
            "step :  26400 iters_100_time :  76.07829689979553\n",
            "step :  26500 iters_100_time :  75.40606737136841\n",
            "[390/600][442]\n",
            "                    Loss_D: 3.84 Loss_G: 65.24 Time: 335.50s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  26600 iters_100_time :  61.57484173774719\n",
            "step :  26700 iters_100_time :  75.42374205589294\n",
            "step :  26800 iters_100_time :  75.79042196273804\n",
            "step :  26900 iters_100_time :  75.53203225135803\n",
            "[391/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 61.42 Time: 335.51s\n",
            "num_batches :  442\n",
            "step :  27000 iters_100_time :  29.09303379058838\n",
            "step :  27100 iters_100_time :  75.73178267478943\n",
            "step :  27200 iters_100_time :  76.29932618141174\n",
            "step :  27300 iters_100_time :  75.60762739181519\n",
            "step :  27400 iters_100_time :  75.60378122329712\n",
            "[392/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 46.76 Time: 335.74s\n",
            "num_batches :  442\n",
            "step :  27500 iters_100_time :  73.45606851577759\n",
            "step :  27600 iters_100_time :  75.80700635910034\n",
            "step :  27700 iters_100_time :  75.79052209854126\n",
            "step :  27800 iters_100_time :  75.59198832511902\n",
            "[393/600][442]\n",
            "                    Loss_D: 0.34 Loss_G: 41.72 Time: 335.68s\n",
            "num_batches :  442\n",
            "step :  27900 iters_100_time :  41.55634069442749\n",
            "step :  28000 iters_100_time :  75.3282356262207\n",
            "step :  28100 iters_100_time :  75.5370225906372\n",
            "step :  28200 iters_100_time :  76.01316809654236\n",
            "[394/600][442]\n",
            "                    Loss_D: 0.18 Loss_G: 58.98 Time: 335.12s\n",
            "num_batches :  442\n",
            "step :  28300 iters_100_time :  9.683844089508057\n",
            "step :  28400 iters_100_time :  75.6659836769104\n",
            "step :  28500 iters_100_time :  75.45365881919861\n",
            "step :  28600 iters_100_time :  75.9052460193634\n",
            "step :  28700 iters_100_time :  76.16669368743896\n",
            "[395/600][442]\n",
            "                    Loss_D: 0.18 Loss_G: 50.44 Time: 335.61s\n",
            "num_batches :  442\n",
            "step :  28800 iters_100_time :  53.58432173728943\n",
            "step :  28900 iters_100_time :  75.57595348358154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBwYuwliPq-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}